{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EFFICIENTNETB0 MODEL\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# Define dataset path\n",
    "DATASET_PATH = \"/kaggle/input/glaucomadataset-healthy-and-infected-images/datasets combined/\"\n",
    "\n",
    "# Image Augmentation to increase dataset size\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# Load dataset\n",
    "train_data = train_datagen.flow_from_directory(\n",
    "    DATASET_PATH,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=8,  # Smaller batch size for small datasets\n",
    "    class_mode='binary',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_data = train_datagen.flow_from_directory(\n",
    "    DATASET_PATH,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=8,\n",
    "    class_mode='binary',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Load EfficientNetB0 Pre-trained Model\n",
    "base_model = EfficientNetB0(\n",
    "    weights=\"/kaggle/input/efficientnetb0-notop-h5/efficientnetb0_notop.h5\",\n",
    "    include_top=False,\n",
    "    input_shape=(224, 224, 3)\n",
    ")\n",
    "\n",
    "# Add Custom Layers on top\n",
    "x = GlobalAveragePooling2D()(base_model.output)\n",
    "x = Dropout(0.4)(x)  # Reduce overfitting\n",
    "x = Dense(128, activation=\"relu\")(x)\n",
    "x = Dropout(0.3)(x)  # More dropout\n",
    "output = Dense(1, activation=\"sigmoid\")(x)  # Binary classification\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "# Compile the Model\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.0001),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.2, patience=3, min_lr=1e-6)\n",
    "\n",
    "# Train the Model\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    validation_data=val_data,\n",
    "    epochs=30,\n",
    "    callbacks=[early_stopping, reduce_lr]\n",
    ")\n",
    "\n",
    "# Save the model\n",
    "model.save(\"glaucoma_model.keras\")\n",
    "\n",
    "print(\"Training completed and model saved!\")\n",
    "\n",
    "# Compute the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Make predictions on the validation set\n",
    "predicted_classes_efficientnet = model.predict(X_test)  # Replace cnn_model with your actual CNN model variable\n",
    "predicted_classes_efficientnet = np.argmax(predicted_classes_efficientnet, axis=1)  # In case of multi-class classification, use argmax\n",
    "\n",
    "efficientnet_accuracy = accuracy_score(y_test, predicted_classes_cnn)  # Now using the defined predicted_classes_cnn\n",
    "efficientnet_report = classification_report(y_test, predicted_classes_efficientnet, output_dict=True)\n",
    "efficientnet_precision = efficientnet_report['accuracy']\n",
    "efficientnet_recall = efficientnet_report['macro avg']['recall']\n",
    "efficientnet_f1 = efficientnet_report['macro avg']['f1-score']\n",
    "\n",
    "# Add the results to the list\n",
    "results.append({\n",
    "    \"Model\": \"efficientnet\",\n",
    "    \"Accuracy\": efficient_accuracy,\n",
    "    \"Precision\": efficient_precision,\n",
    "    \"Recall\": efficient_recall,\n",
    "    \"F1-Score\": efficient_f1\n",
    "})\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()\n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Model Accuracy')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Model Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MOBILENETV2 MODEL\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras import layers, models, optimizers, callbacks\n",
    "\n",
    "# Define paths\n",
    "data_dir = \"/kaggle/input/glaucomadataset-healthy-and-infected-images/datasets combined\"\n",
    "\n",
    "# Image Data Augmentation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# Load Training and Validation Data\n",
    "train_data = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_data = train_datagen.flow_from_directory(\n",
    "    data_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='binary',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Load MobileNetV2 Pre-trained Model\n",
    "base_model = MobileNetV2(weights=\"/kaggle/input/mobilenet/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\",\n",
    "                                 include_top=False, input_shape=(224, 224, 3))\n",
    "\n",
    "\n",
    "# Build Model\n",
    "model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(1, activation='sigmoid')  # Binary classification (Healthy vs Glaucoma)\n",
    "])\n",
    "\n",
    "# Compile Model\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=0.0001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Callbacks (Early Stopping & Reduce LR on Plateau)\n",
    "\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3)\n",
    "\n",
    "# Train Model\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    validation_data=val_data,\n",
    "    epochs=17,\n",
    "    callbacks=[reduce_lr]\n",
    ")\n",
    "\n",
    "# Save Model\n",
    "model.save(\"mobilenetv2_glaucoma_model.keras\")\n",
    "\n",
    "print(\"Training completed and model saved!\")\n",
    "\n",
    "# Compute the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "# Make predictions on the validation set\n",
    "predictions = model.predict(val_data)  # Using validation set for predictions\n",
    "\n",
    "# Convert probabilities to class labels\n",
    "predicted_classes = (predictions > 0.5).astype(\"int32\")  # Threshold at 0.5 for binary classification\n",
    "\n",
    "# Assuming val_data has the true labels as well\n",
    "true_labels = val_data.classes\n",
    "\n",
    "cm = confusion_matrix(true_labels, predicted_classes)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "report = classification_report(true_labels, predicted_classes, target_names=['Healthy', 'Glaucoma'])\n",
    "print(\"Classification Report:\\n\", report)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()\n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Model Accuracy')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Model Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DENSENET121\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from PIL import Image\n",
    "\n",
    "# Define paths\n",
    "dataset_path = \"/kaggle/input/glaucomadataset-healthy-and-infected-images/datasets combined\"\n",
    "batch_size = 16  # Adjust based on Kaggle memory limits\n",
    "img_size = (224, 224)  # Resize images for DenseNet\n",
    "\n",
    "# Data augmentation and normalization\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2  # Splitting into training and validation\n",
    ")\n",
    "\n",
    "# Load training images\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "# Load validation images\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Define the correct path to your local weights file\n",
    "weights_path = \"/kaggle/input/dense121/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "\n",
    "# Load DenseNet121 model without the top layers (for feature extraction)\n",
    "base_model = tf.keras.applications.DenseNet121(\n",
    "    include_top=False,  # Remove the fully connected layers\n",
    "    input_shape=(224, 224, 3),  # Adjust the input shape if needed\n",
    "    weights=None  # Do not fetch from the internet\n",
    ")\n",
    "\n",
    "# Load the weights from the local file\n",
    "base_model.load_weights(weights_path)\n",
    "\n",
    "# Set the base model to not trainable (freeze it)\n",
    "base_model.trainable = False\n",
    "\n",
    "# Add custom layers on top of the base model\n",
    "x = base_model.output\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)  # Pooling layer to flatten the output\n",
    "x = tf.keras.layers.Dense(1024, activation='relu')(x)  # Fully connected layer\n",
    "x = tf.keras.layers.Dropout(0.5)(x)  # Dropout layer to reduce overfitting\n",
    "predictions = tf.keras.layers.Dense(1, activation='sigmoid')(x)  # 1 class: healthy or glaucoma (binary classification)\n",
    "\n",
    "# Final model\n",
    "model = tf.keras.models.Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model with binary_crossentropy loss function\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "              loss='binary_crossentropy',  # Use binary_crossentropy for binary classification\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model using the generators\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=15,  # Set the number of epochs for training\n",
    "    validation_data=val_generator,\n",
    "    verbose=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADJUSTING DENSENET121 THROUGH\n",
    "# 1.DATA AUGUMENTATION AND L2 REGULARIZATION\n",
    "# 2.DROPOUT\n",
    "# 3.LEARNING RATE REDUCTION AND EARLY STOPPING\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import DenseNet121\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras import regularizers\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# Define paths\n",
    "dataset_path = \"/kaggle/input/glaucomadataset-healthy-and-infected-images/datasets combined\"\n",
    "batch_size = 16  # Adjust based on Kaggle memory limits\n",
    "img_size = (224, 224)  # Resize images for DenseNet\n",
    "\n",
    "# Data augmentation and normalization\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    validation_split=0.2  # Splitting into training and validation\n",
    ")\n",
    "\n",
    "# Load training images\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "# Load validation images\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    dataset_path,\n",
    "    target_size=img_size,\n",
    "    batch_size=batch_size,\n",
    "    class_mode='binary',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "# Define the correct path to your local weights file\n",
    "weights_path = \"/kaggle/input/dense121/densenet121_weights_tf_dim_ordering_tf_kernels_notop.h5\"\n",
    "\n",
    "# Load DenseNet121 model without the top layers (for feature extraction)\n",
    "base_model = tf.keras.applications.DenseNet121(\n",
    "    include_top=False,  # Remove the fully connected layers\n",
    "    input_shape=(224, 224, 3),  # Adjust the input shape if needed\n",
    "    weights=None  # Do not fetch from the internet\n",
    ")\n",
    "\n",
    "# Load the weights from the local file\n",
    "base_model.load_weights(weights_path)\n",
    "\n",
    "# Set the base model to not trainable (freeze it)\n",
    "base_model.trainable = False\n",
    "\n",
    "# Add custom layers on top of the base model\n",
    "x = base_model.output\n",
    "x = tf.keras.layers.GlobalAveragePooling2D()(x)  # Pooling layer to flatten the output\n",
    "x = tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)  # L2 Regularization\n",
    "x = tf.keras.layers.Dropout(0.5)(x)  # Dropout layer to reduce overfitting\n",
    "predictions = tf.keras.layers.Dense(1, activation='sigmoid')(x)  # 1 class: healthy or glaucoma (binary classification)\n",
    "\n",
    "# Final model\n",
    "model = tf.keras.models.Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Compile the model with binary_crossentropy loss function\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "              loss='binary_crossentropy',  # Use binary_crossentropy for binary classification\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Early stopping and reduce learning rate on plateau\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2, min_lr=1e-6)\n",
    "\n",
    "# Train the model using the generators\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=13,  # Set the number of epochs for training\n",
    "    validation_data=val_generator,\n",
    "    verbose=1,\n",
    "    callbacks=[early_stopping, reduce_lr]  # Use EarlyStopping and Learning Rate Reduction\n",
    ")\n",
    "# Make predictions on the validation set\n",
    "predictions = model.predict(val_generator)  # Using validation set for predictions\n",
    "\n",
    "# Convert probabilities to class labels (threshold at 0.5 for binary classification)\n",
    "predicted_classes = (predictions > 0.5).astype(\"int32\")\n",
    "\n",
    "# Compute the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming val_generator has the true labels as well\n",
    "true_labels = val_generator.classes\n",
    "\n",
    "cm = confusion_matrix(true_labels, predicted_classes)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "\n",
    "# Classification Report (Precision, Recall, F1-score)\n",
    "report = classification_report(true_labels, predicted_classes, target_names=['Healthy', 'Glaucoma'])\n",
    "print(\"Classification Report:\\n\", report)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.show()\n",
    "\n",
    "# Plotting accuracy and loss\n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Model Accuracy')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Model Loss')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN MODEL\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
    "\n",
    "# Set image size\n",
    "IMG_SIZE = 224\n",
    "\n",
    "# Define paths\n",
    "healthy_path = \"/kaggle/input/glaucomadataset-healthy-and-infected-images/datasets combined/healthy\"\n",
    "glaucoma_path = \"/kaggle/input/glaucomadataset-healthy-and-infected-images/datasets combined/glaucoma_project\"\n",
    "\n",
    "# Initialize data lists\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "# Load healthy images (Label = 0)\n",
    "for img_name in os.listdir(healthy_path):\n",
    "    img_path = os.path.join(healthy_path, img_name)\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is not None:  # Check if image is valid\n",
    "        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE)) / 255.0  # Normalize\n",
    "        X.append(img)\n",
    "        y.append(0)  # Label for healthy\n",
    "\n",
    "# Load glaucoma-infected images (Label = 1)\n",
    "for img_name in os.listdir(glaucoma_path):\n",
    "    img_path = os.path.join(glaucoma_path, img_name)\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is not None:\n",
    "        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE)) / 255.0\n",
    "        X.append(img)\n",
    "        y.append(1)  # Label for glaucoma\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Split into training (80%) and testing (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Data Augmentation to artificially increase dataset size\n",
    "datagen = ImageDataGenerator(\n",
    "    rescale=1./255,           # Normalize pixel values\n",
    "    rotation_range=30,        # Random rotation\n",
    "    width_shift_range=0.2,    # Horizontal shift\n",
    "    height_shift_range=0.2,   # Vertical shift\n",
    "    shear_range=0.2,          # Shearing\n",
    "    zoom_range=0.2,           # Zooming\n",
    "    horizontal_flip=True,     # Flipping images\n",
    "    fill_mode='nearest'       # Fill in missing pixels\n",
    ")\n",
    "\n",
    "# CNN model\n",
    "model = Sequential([\n",
    "    Input(shape=(IMG_SIZE, IMG_SIZE, 3)),  # Define input shape separately\n",
    "\n",
    "    # First Convolutional Layer\n",
    "    Conv2D(32, (3,3), activation=\"relu\"),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "\n",
    "    # Second Convolutional Layer\n",
    "    Conv2D(64, (3,3), activation=\"relu\"),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "\n",
    "    # Third Convolutional Layer\n",
    "    Conv2D(128, (3,3), activation=\"relu\"),\n",
    "    MaxPooling2D(pool_size=(2,2)),\n",
    "\n",
    "    # Flatten and Fully Connected Layers\n",
    "    Flatten(),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dropout(0.5),  # Prevent overfitting\n",
    "    Dense(1, activation=\"sigmoid\")  # Output layer for binary classification\n",
    "])\n",
    "\n",
    "# Choose optimizer (You can switch between 'adam' and 'rmsprop')\n",
    "optimizer = Adam(learning_rate=0.001)  # You can change to RMSprop(learning_rate=0.001)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# ReduceLROnPlateau - Automatically adjusts learning rate if model stops improving\n",
    "lr_reduction = ReduceLROnPlateau(monitor='val_loss', patience=3, factor=0.5, min_lr=0.00001)\n",
    "\n",
    "# Train the model with data augmentation\n",
    "history = model.fit(\n",
    "    datagen.flow(X_train, y_train, batch_size=4),  # Apply data augmentation\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=15,  # Increased from default (Try adjusting)\n",
    "    callbacks=[lr_reduction],  # Reduce learning rate when necessary\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# CNN Model - ensure this part gets defined after training the CNN model\n",
    "predicted_classes_cnn = model.predict(X_test)  # Replace cnn_model with your actual CNN model variable\n",
    "predicted_classes_cnn = np.argmax(predicted_classes_cnn, axis=1)  # In case of multi-class classification, use argmax\n",
    "\n",
    "cnn_accuracy = accuracy_score(y_test, predicted_classes_cnn)  # Now using the defined predicted_classes_cnn\n",
    "cnn_report = classification_report(y_test, predicted_classes_cnn, output_dict=True)\n",
    "cnn_precision = cnn_report['accuracy']\n",
    "cnn_recall = cnn_report['macro avg']['recall']\n",
    "cnn_f1 = cnn_report['macro avg']['f1-score']\n",
    "\n",
    "# Add the results to the list\n",
    "results.append({\n",
    "    \"Model\": \"CNN\",\n",
    "    \"Accuracy\": cnn_accuracy,\n",
    "    \"Precision\": cnn_precision,\n",
    "    \"Recall\": cnn_recall,\n",
    "    \"F1-Score\": cnn_f1\n",
    "})\n",
    "\n",
    "\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Plot training accuracy and loss\n",
    "plt.figure(figsize=(12,4))\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Model Accuracy')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title('Model Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOGISTIC REGRESSION MODEL\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Define paths\n",
    "healthy_path = \"/kaggle/input/glaucomadataset-healthy-and-infected-images/datasets combined/healthy\"\n",
    "glaucoma_path = \"/kaggle/input/glaucomadataset-healthy-and-infected-images/datasets combined/glaucoma_project\"\n",
    "\n",
    "# Image settings\n",
    "IMG_SIZE = 224  # Resize images to 224x224\n",
    "\n",
    "# Prepare data storage\n",
    "X = []  # Image data\n",
    "y = []  # Labels (0 = Healthy, 1 = Glaucoma)\n",
    "\n",
    "# Load healthy images\n",
    "for img_name in os.listdir(healthy_path):\n",
    "    img = cv2.imread(os.path.join(healthy_path, img_name))\n",
    "    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE)) / 255.0  # Normalize\n",
    "    X.append(img)\n",
    "    y.append(0)  # Label for healthy\n",
    "\n",
    "# Load glaucoma-infected images\n",
    "for img_name in os.listdir(glaucoma_path):\n",
    "    img = cv2.imread(os.path.join(glaucoma_path, img_name))\n",
    "    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE)) / 255.0\n",
    "    X.append(img)\n",
    "    y.append(1)  # Label for glaucoma\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Split into training (80%) and testing (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Total images: {len(X)}, Training: {len(X_train)}, Testing: {len(X_test)}\")\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Flatten images for logistic regression (convert 2D to 1D)\n",
    "X_train_flat = X_train.reshape(len(X_train), -1)\n",
    "X_test_flat = X_test.reshape(len(X_test), -1)\n",
    "\n",
    "# Train Logistic Regression model\n",
    "log_model = LogisticRegression(max_iter=1000)\n",
    "log_model.fit(X_train_flat, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = log_model.predict(X_test_flat)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Logistic Regression\n",
    "log_cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Logistic Regression Confusion Matrix:\\n\", log_cm)\n",
    "print(\"Logistic Regression Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "print(f\"Baseline Model (Logistic Regression) Accuracy: {accuracy:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
