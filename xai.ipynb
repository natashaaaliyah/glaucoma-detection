{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "###XAI for Random Forest(seconf best)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import shap\n",
    "import lime.lime_tabular\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.inspection import permutation_importance, PartialDependenceDisplay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# -------------------------------\n",
    "# ðŸ”¹ Load Data and Preprocess\n",
    "# -------------------------------\n",
    "data = pd.read_csv(\"/kaggle/input/glaucoma/glaucoma_dataset.csv\")\n",
    "X = data.drop('Diagnosis', axis=1).select_dtypes(include=['number']).fillna(0)\n",
    "y = (data['Diagnosis'] == 'Glaucoma').astype(int)\n",
    "\n",
    "# Convert DataFrame to NumPy array for model compatibility\n",
    "X_numpy = X.values\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_numpy, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale Data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Load Pretrained Random Forest Model\n",
    "rf_model = joblib.load(\"glaucoma_model_Random_Forest.joblib\")\n",
    "\n",
    "# Select a Sample for Explanation\n",
    "sample_idx = 10  # Pick a random test sample\n",
    "X_sample = X_test[sample_idx].reshape(1, -1)  # Convert single instance to NumPy array\n",
    "\n",
    "# -------------------------------\n",
    "# ðŸ”¹ 1. Feature Importance (Random Forest)\n",
    "# -------------------------------\n",
    "feature_importance = pd.Series(rf_model.named_steps['model'].feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=feature_importance[:10], y=feature_importance.index[:10], palette=\"coolwarm\")\n",
    "plt.xlabel(\"Feature Importance Score\")\n",
    "plt.title(\"Top 10 Feature Importance (Random Forest)\")\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# ðŸ”¹ 2. Partial Dependence Plot (PDP) - FIXED\n",
    "# -------------------------------\n",
    "# Use only features present in the dataset\n",
    "valid_features = [f for f in feature_importance.index if f in X.columns][:2]  # Pick first 2 valid features\n",
    "\n",
    "if valid_features:\n",
    "    display = PartialDependenceDisplay.from_estimator(rf_model.named_steps['model'], X_train_scaled, features=[X.columns.get_loc(f) for f in valid_features], grid_resolution=50)\n",
    "    display.plot()\n",
    "    plt.suptitle(\"Partial Dependence Plots (Top 2 Features)\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No valid features found for Partial Dependence Plot.\")\n",
    "\n",
    "# -------------------------------\n",
    "# ðŸ”¹ 3. Permutation Importance\n",
    "# -------------------------------\n",
    "perm_importance = permutation_importance(rf_model, X_test_scaled, y_test, scoring='accuracy', n_repeats=10, random_state=42)\n",
    "perm_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': perm_importance.importances_mean})\n",
    "perm_importance_df = perm_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=perm_importance_df[\"Importance\"][:10], y=perm_importance_df[\"Feature\"][:10], palette=\"coolwarm\")\n",
    "plt.xlabel(\"Permutation Importance Score\")\n",
    "plt.title(\"Top 10 Permutation Importance (Random Forest)\")\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# ðŸ”¹ 4. LIME Explanation (for one sample)\n",
    "# -------------------------------\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(X_train_scaled, feature_names=X.columns, class_names=['No Glaucoma', 'Glaucoma'], discretize_continuous=True)\n",
    "exp = explainer.explain_instance(X_sample[0], rf_model.predict_proba, num_features=5)\n",
    "exp.show_in_notebook()\n",
    "\n",
    "# -------------------------------\n",
    "# ðŸ”¹ 5. SHAP Explanation (Limited Visualizations)\n",
    "# -------------------------------\n",
    "explainer = shap.Explainer(rf_model.predict_proba, X_train_scaled)\n",
    "shap_values = explainer(X_test_scaled)\n",
    "\n",
    "# Summary Plot (Top 10 features only)\n",
    "shap.summary_plot(shap_values[..., 1], X_test_scaled, feature_names=X.columns, max_display=10)  # Class 1 (Glaucoma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###XAI on SVM (second best)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import shap\n",
    "import lime.lime_tabular\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.inspection import permutation_importance, PartialDependenceDisplay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# -------------------------------\n",
    "# ðŸ”¹ Load Data and Preprocess\n",
    "# -------------------------------\n",
    "data = pd.read_csv(\"/kaggle/input/glaucoma/glaucoma_dataset.csv\")\n",
    "X = data.drop('Diagnosis', axis=1).select_dtypes(include=['number']).fillna(0)\n",
    "y = (data['Diagnosis'] == 'Glaucoma').astype(int)\n",
    "\n",
    "# Convert DataFrame to NumPy array for model compatibility\n",
    "X_numpy = X.values\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_numpy, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale Data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Load Pretrained SVM Model\n",
    "svm_model = joblib.load(\"glaucoma_model_SVM.joblib\")\n",
    "\n",
    "# Select a Sample for Explanation\n",
    "sample_idx = 10  # Pick a random test sample\n",
    "X_sample = X_test[sample_idx].reshape(1, -1)  # Convert single instance to NumPy array\n",
    "\n",
    "# -------------------------------\n",
    "# ðŸ”¹ 1. Feature Importance via Permutation Importance (SVM has no built-in feature importance)\n",
    "# -------------------------------\n",
    "perm_importance = permutation_importance(svm_model, X_test_scaled, y_test, scoring='accuracy', n_repeats=10, random_state=42)\n",
    "perm_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': perm_importance.importances_mean})\n",
    "perm_importance_df = perm_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=perm_importance_df[\"Importance\"][:10], y=perm_importance_df[\"Feature\"][:10], palette=\"coolwarm\")\n",
    "plt.xlabel(\"Permutation Importance Score\")\n",
    "plt.title(\"Top 10 Feature Importance (SVM)\")\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# ðŸ”¹ 2. Partial Dependence Plot (PDP) - FIXED\n",
    "# -------------------------------\n",
    "# Use only features present in the dataset\n",
    "valid_features = perm_importance_df[\"Feature\"][:2].tolist()  # Pick first 2 valid features\n",
    "\n",
    "if valid_features:\n",
    "    display = PartialDependenceDisplay.from_estimator(svm_model, X_train_scaled, features=[X.columns.get_loc(f) for f in valid_features], grid_resolution=50)\n",
    "    display.plot()\n",
    "    plt.suptitle(\"Partial Dependence Plots (Top 2 Features)\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No valid features found for Partial Dependence Plot.\")\n",
    "\n",
    "# -------------------------------\n",
    "# ðŸ”¹ 3. Permutation Importance (Direct Interpretation)\n",
    "# -------------------------------\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=perm_importance_df[\"Importance\"][:10], y=perm_importance_df[\"Feature\"][:10], palette=\"coolwarm\")\n",
    "plt.xlabel(\"Permutation Importance Score\")\n",
    "plt.title(\"Top 10 Permutation Importance (SVM Model)\")\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# ðŸ”¹ 4. LIME Explanation (for one sample)\n",
    "# -------------------------------\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(X_train_scaled, feature_names=X.columns, class_names=['No Glaucoma', 'Glaucoma'], discretize_continuous=True)\n",
    "exp = explainer.explain_instance(X_sample[0], svm_model.decision_function, num_features=5)\n",
    "exp.show_in_notebook()\n",
    "\n",
    "# -------------------------------\n",
    "# ðŸ”¹ 5. SHAP Explanation (Limited Visualizations)\n",
    "# -------------------------------\n",
    "explainer = shap.Explainer(svm_model.decision_function, X_train_scaled)\n",
    "shap_values = explainer(X_test_scaled)\n",
    "\n",
    "# Summary Plot (Top 10 features only)\n",
    "shap.summary_plot(shap_values, X_test_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### XAI on knn model(best performing model)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import shap\n",
    "\n",
    "import lime.lime_tabular\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance, PartialDependenceDisplay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# -------------------------------\n",
    "# ðŸ”¹ Load Data and Preprocess\n",
    "# -------------------------------\n",
    "data = pd.read_csv(\"/kaggle/input/glaucoma/glaucoma_dataset.csv\")\n",
    "X = data.drop('Diagnosis', axis=1).select_dtypes(include=['number']).fillna(0)\n",
    "y = (data['Diagnosis'] == 'Glaucoma').astype(int)\n",
    "\n",
    "# Convert DataFrame to NumPy array for KNN model\n",
    "X_numpy = X.values\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_numpy, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale Data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Load Pretrained KNN Model\n",
    "knn_model = joblib.load(\"glaucoma_model_KNN.joblib\")\n",
    "\n",
    "# Train a Random Forest for Feature Importance (since KNN lacks built-in feature importance)\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Select a Sample for Explanation\n",
    "sample_idx = 10  # Pick a random test sample\n",
    "X_sample = X_test[sample_idx].reshape(1, -1)  # Convert single instance to NumPy array\n",
    "\n",
    "# -------------------------------\n",
    "# ðŸ”¹ 1. Feature Importance (Random Forest)\n",
    "# -------------------------------\n",
    "feature_importance = pd.Series(rf_model.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=feature_importance[:10], y=feature_importance.index[:10], palette=\"coolwarm\")\n",
    "plt.xlabel(\"Feature Importance Score\")\n",
    "plt.title(\"Top 10 Feature Importance (Random Forest)\")\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# ðŸ”¹ 2. Partial Dependence Plot (PDP) - FIXED\n",
    "# -------------------------------\n",
    "# Use only features present in the scaled dataset\n",
    "valid_features = [f for f in feature_importance.index if f in X.columns][:2]  # Pick the first 2 valid features\n",
    "\n",
    "if valid_features:\n",
    "    display = PartialDependenceDisplay.from_estimator(rf_model, X_train_scaled, features=[X.columns.get_loc(f) for f in valid_features], grid_resolution=50)\n",
    "    display.plot()\n",
    "    plt.suptitle(\"Partial Dependence Plots (Top 2 Features)\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No valid features found for Partial Dependence Plot.\")\n",
    "\n",
    "# -------------------------------\n",
    "# ðŸ”¹ 2. Partial Dependence Plot (PDP) - FIXED\n",
    "# -------------------------------\n",
    "# Use only features present in the scaled dataset\n",
    "valid_features = [f for f in feature_importance.index if f in X.columns][:2]  # Pick the first 2 valid features\n",
    "\n",
    "if valid_features:\n",
    "    display = PartialDependenceDisplay.from_estimator(rf_model, X_train_scaled, features=[X.columns.get_loc(f) for f in valid_features], grid_resolution=50)\n",
    "    display.plot()\n",
    "    plt.suptitle(\"Partial Dependence Plots (Top 2 Features)\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No valid features found for Partial Dependence Plot.\")\n",
    "# ðŸ”¹ 3. Permutation Importance\n",
    "# -------------------------------\n",
    "perm_importance = permutation_importance(knn_model, X_test_scaled, y_test, scoring='accuracy', n_repeats=10, random_state=42)\n",
    "perm_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': perm_importance.importances_mean})\n",
    "perm_importance_df = perm_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=perm_importance_df[\"Importance\"][:10], y=perm_importance_df[\"Feature\"][:10], palette=\"coolwarm\")\n",
    "plt.xlabel(\"Permutation Importance Score\")\n",
    "plt.title(\"Top 10 Permutation Importance (KNN Model)\")\n",
    "plt.show()\n",
    "# -------------------------------\n",
    "# ðŸ”¹ 4. LIME Explanation (for one sample)\n",
    "# -------------------------------\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(X_train_scaled, feature_names=X.columns, class_names=['No Glaucoma', 'Glaucoma'], discretize_continuous=True)\n",
    "exp = explainer.explain_instance(X_sample[0], knn_model.predict_proba, num_features=5)\n",
    "exp.show_in_notebook()\n",
    "# ðŸ”¹ 5. SHAP Explanation (Limited Visualizations)\n",
    "# -------------------------------\n",
    "explainer = shap.Explainer(knn_model.predict_proba, X_train_scaled)\n",
    "shap_values = explainer(X_test_scaled)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
