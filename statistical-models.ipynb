{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-23T11:13:15.322009Z",
     "iopub.status.busy": "2025-03-23T11:13:15.321694Z",
     "iopub.status.idle": "2025-03-23T11:17:03.81531Z",
     "shell.execute_reply": "2025-03-23T11:17:03.814358Z",
     "shell.execute_reply.started": "2025-03-23T11:13:15.321986Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "import json\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "import joblib\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.base import clone  # Import clone from sklearn\n",
    "\n",
    "class GlaucomaDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.FloatTensor(X)\n",
    "        self.y = torch.LongTensor(y)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size, encoding_dim=64):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        \n",
    "        # Encoder layers\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, encoding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # Decoder layers\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, input_size),\n",
    "            nn.Sigmoid()  # Output should be in the range [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "def train_autoencoder(model, train_loader, optimizer, criterion, device, num_epochs=50):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        train_loss = 0\n",
    "        for batch_X, _ in train_loader:\n",
    "            batch_X = batch_X.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(batch_X)\n",
    "            loss = criterion(outputs, batch_X)\n",
    "            \n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Autoencoder Loss: {train_loss/len(train_loader):.4f}')\n",
    "\n",
    "def train_and_evaluate_models(X_train, X_val, y_train, y_val, scaler_mean, scaler_scale):\n",
    "    # First apply PCA\n",
    "    pca = PCA(n_components=0.95)  # Keep 95% of variance\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    X_val_pca = pca.transform(X_val)\n",
    "    \n",
    "    results = {}\n",
    "    models = {\n",
    "        # 'Linear_Regression': Pipeline([\n",
    "        #     ('model', LinearRegression())\n",
    "        # ]),\n",
    "        'Logistic_Regression': Pipeline([\n",
    "            ('model', LogisticRegression(max_iter=2000))\n",
    "        ]),\n",
    "        'Decision_Tree': Pipeline([\n",
    "            ('model', DecisionTreeClassifier(random_state=42))\n",
    "        ]),\n",
    "        'Random_Forest': Pipeline([\n",
    "            ('model', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "        ]),\n",
    "        'SVM': Pipeline([\n",
    "            ('model', SVC(probability=True, kernel='rbf', random_state=42))\n",
    "        ]),\n",
    "        'KNN': Pipeline([\n",
    "            ('model', KNeighborsClassifier(n_neighbors=5))\n",
    "        ]),\n",
    "        'GBM': Pipeline([\n",
    "            ('model', GradientBoostingClassifier(random_state=42))\n",
    "        ]),\n",
    "        'XGBoost': Pipeline([\n",
    "            ('model', XGBClassifier(random_state=42,\n",
    "                                     learning_rate=0.1,\n",
    "                                     n_estimators=100,\n",
    "                                     max_depth=5))\n",
    "        ]),\n",
    "        'Neural_Network': Pipeline([\n",
    "            ('model', MLPClassifier(hidden_layer_sizes=(64, 32),\n",
    "                                    max_iter=1000,\n",
    "                                    random_state=42))\n",
    "        ])\n",
    "    }\n",
    "\n",
    "    # Train and evaluate each model on both original and PCA features\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        \n",
    "        # Train on original features\n",
    "        model.fit(X_train, y_train)\n",
    "        metrics_orig = evaluate_model(model, X_val, y_val)\n",
    "        results[f'{name}_original'] = metrics_orig\n",
    "        joblib.dump(model, f'glaucoma_model_{name}_original.joblib')\n",
    "        \n",
    "        # Train on PCA-reduced features using sklearn's clone\n",
    "        model_pca = clone(model)\n",
    "        model_pca.fit(X_train_pca, y_train)\n",
    "        metrics_pca = evaluate_model(model_pca, X_val_pca, y_val)\n",
    "        results[f'{name}_pca'] = metrics_pca\n",
    "        joblib.dump((model_pca, pca), f'glaucoma_model_{name}_pca.joblib')\n",
    "    \n",
    "    return results\n",
    "\n",
    "def evaluate_model(model, X, y):\n",
    "    \"\"\"Helper function to evaluate a model\"\"\"\n",
    "    y_pred = model.predict(X)\n",
    "    y_prob = model.predict_proba(X)[:, 1] if hasattr(model, 'predict_proba') else y_pred\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy_score(y, y_pred),\n",
    "        'precision': precision_score(y, y_pred),\n",
    "        'recall': recall_score(y, y_pred),\n",
    "        'f1': f1_score(y, y_pred),\n",
    "        'auc': roc_auc_score(y, y_prob)\n",
    "    }\n",
    "\n",
    "def preprocess_data(data):\n",
    "    def extract_number(value):\n",
    "        if pd.isna(value):\n",
    "            return np.nan\n",
    "        # Extract the first number found in the string\n",
    "        import re\n",
    "        numbers = re.findall(r'[-+]?\\d*\\.\\d+|\\d+', str(value))\n",
    "        return float(numbers[0]) if numbers else np.nan\n",
    "    \n",
    "    # Select and preprocess numerical features\n",
    "    numerical_features = {\n",
    "        'Age': 'Age',\n",
    "        'Visual Acuity': 'Visual Acuity Measurements',\n",
    "        'IOP': 'Intraocular Pressure (IOP)',\n",
    "        'CDR': 'Cup-to-Disc Ratio (CDR)',\n",
    "        'RNFL': 'Optical Coherence Tomography (OCT) Results',\n",
    "        'Pachymetry': 'Pachymetry'\n",
    "    }\n",
    "    \n",
    "    processed_data = pd.DataFrame()\n",
    "    for new_name, original_name in numerical_features.items():\n",
    "        processed_data[new_name] = data[original_name].apply(extract_number)\n",
    "    \n",
    "    # Handle Visual Field Test Results separately since it has sensitivity/specificity\n",
    "    vf_data = data['Visual Field Test Results'].str.extract(r'Sensitivity: ([\\d.]+), Specificity: ([\\d.]+)')\n",
    "    processed_data['VF_Sensitivity'] = pd.to_numeric(vf_data[0])\n",
    "    processed_data['VF_Specificity'] = pd.to_numeric(vf_data[1])\n",
    "    \n",
    "    # Handle OCT measurements\n",
    "    oct_data = data['Optical Coherence Tomography (OCT) Results'].str.extract(\n",
    "        r'RNFL Thickness: ([\\d.]+).*GCC Thickness: ([\\d.]+).*Retinal Volume: ([\\d.]+).*Macular Thickness: ([\\d.]+)'\n",
    "    )\n",
    "    processed_data['RNFL_Thickness'] = pd.to_numeric(oct_data[0])\n",
    "    processed_data['GCC_Thickness'] = pd.to_numeric(oct_data[1])\n",
    "    processed_data['Retinal_Volume'] = pd.to_numeric(oct_data[2])\n",
    "    processed_data['Macular_Thickness'] = pd.to_numeric(oct_data[3])\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "def main():\n",
    "    # Data preparation\n",
    "    data = pd.read_csv(\"/kaggle/input/glaucoma/glaucoma_dataset.csv\", skipinitialspace=True)\n",
    "    processed_data = preprocess_data(data)\n",
    "    X = processed_data.values\n",
    "    y = (data['Diagnosis'] == 'Glaucoma').astype(int).values\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    # Scale data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    \n",
    "    # Save scaler parameters\n",
    "    scaler_mean = scaler.mean_\n",
    "    scaler_scale = scaler.scale_\n",
    "    \n",
    "    # Create datasets and dataloaders\n",
    "    train_dataset = GlaucomaDataset(X_train_scaled, y_train)\n",
    "    val_dataset = GlaucomaDataset(X_val_scaled, y_val)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    # Train autoencoder\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    autoencoder = Autoencoder(input_size=X.shape[1]).to(device)\n",
    "    ae_optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n",
    "    ae_criterion = nn.MSELoss()\n",
    "    \n",
    "    print(\"\\nTraining Autoencoder...\")\n",
    "    train_autoencoder(autoencoder, train_loader, ae_optimizer, ae_criterion, device, num_epochs=50)\n",
    "    \n",
    "    # Extract features from autoencoder\n",
    "    with torch.no_grad():\n",
    "        X_train_encoded = autoencoder.encoder(torch.FloatTensor(X_train_scaled).to(device)).cpu().numpy()\n",
    "        X_val_encoded = autoencoder.encoder(torch.FloatTensor(X_val_scaled).to(device)).cpu().numpy()\n",
    "    \n",
    "    # Train classical ML models\n",
    "    print(\"\\nTraining models on original features...\")\n",
    "    ml_results = train_and_evaluate_models(X_train_scaled, X_val_scaled, y_train, y_val, scaler_mean, scaler_scale)\n",
    "    \n",
    "    print(\"\\nTraining models on autoencoder features...\")\n",
    "    ml_results_encoded = train_and_evaluate_models(X_train_encoded, X_val_encoded, y_train, y_val, scaler_mean, scaler_scale)\n",
    "    \n",
    "    # Combine results\n",
    "    ml_results.update({f\"{k}_Encoded\": v for k, v in ml_results_encoded.items()})\n",
    "    \n",
    "    # Print and save all results\n",
    "    print(\"\\nModel Performance Summary:\")\n",
    "    print(\"=\" * 50)\n",
    "    for model_name, metrics in ml_results.items():\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "    # Save metrics to file\n",
    "    with open('model_metrics.json', 'w') as f:\n",
    "        json.dump(ml_results, f, indent=4)\n",
    "    \n",
    "    print(\"\\nMetrics saved to model_metrics.json\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T11:17:03.816951Z",
     "iopub.status.busy": "2025-03-23T11:17:03.816616Z",
     "iopub.status.idle": "2025-03-23T11:17:04.58475Z",
     "shell.execute_reply": "2025-03-23T11:17:04.583985Z",
     "shell.execute_reply.started": "2025-03-23T11:17:03.816919Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "import json\n",
    "\n",
    "def display_metrics_in_terminal(json_path):\n",
    "    with open(json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    table_data = []\n",
    "    for model_name, metrics in data.items():\n",
    "        table_data.append([\n",
    "            model_name,\n",
    "            metrics['accuracy'],\n",
    "            metrics['precision'],\n",
    "            metrics['recall'],\n",
    "            metrics['f1'],\n",
    "            metrics['auc']\n",
    "        ])\n",
    "    \n",
    "    headers = [\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"AUC\"]\n",
    "    print(tabulate(table_data, headers=headers, tablefmt='psql'))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    display_metrics_in_terminal(\"model_metrics.json\")\n",
    "    \n",
    "import json\n",
    "from tabulate import tabulate\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def load_results(filepath):\n",
    "    with open(filepath, 'r') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def create_performance_table(results):\n",
    "    # Prepare data for tabulation\n",
    "    table_data = []\n",
    "    headers = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1', 'AUC']\n",
    "\n",
    "    for model_name, model_data in results.items():\n",
    "        row = [\n",
    "            model_name.upper(),\n",
    "            f\"{model_data['accuracy']:.4f}\",\n",
    "            f\"{model_data['precision']:.4f}\",\n",
    "            f\"{model_data['recall']:.4f}\",\n",
    "            f\"{model_data['f1']:.4f}\",\n",
    "            f\"{model_data['auc']:.4f}\"\n",
    "        ]\n",
    "        table_data.append(row)\n",
    "\n",
    "    # Sort by AUC\n",
    "    table_data.sort(key=lambda x: float(x[5]), reverse=True)\n",
    "    return table_data, headers\n",
    "\n",
    "def plot_comparison(results):\n",
    "    models = list(results.keys())\n",
    "    auc_scores = [results[m]['auc'] for m in models]\n",
    "    f1_scores = [results[m]['f1'] for m in models]\n",
    "\n",
    "    # Create figure with subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "    # Bar plot comparing AUC scores\n",
    "    ax1.bar(models, auc_scores, label='AUC', color='skyblue')\n",
    "    ax1.set_ylabel('AUC Score')\n",
    "    ax1.set_title('AUC Scores by Model')\n",
    "    ax1.set_xticks(range(len(models)))\n",
    "    ax1.set_xticklabels([m.upper() for m in models], rotation=45)\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Bar plot showing F1 scores\n",
    "    ax2.bar(models, f1_scores, color='lightgreen')\n",
    "    ax2.set_ylabel('F1 Score')\n",
    "    ax2.set_title('F1 Scores by Model')\n",
    "    ax2.set_xticks(range(len(models)))\n",
    "    ax2.set_xticklabels([m.upper() for m in models], rotation=45)\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def main():\n",
    "    # Load results\n",
    "    results = load_results('model_metrics.json')\n",
    "\n",
    "    # Create and display performance table\n",
    "    table_data, headers = create_performance_table(results)\n",
    "    print(\"\\nModel Performance Summary:\")\n",
    "    print(tabulate(table_data, headers=headers, tablefmt='grid'))\n",
    "\n",
    "    # Print summary statistics\n",
    "    print(\"\\nSummary Statistics:\")\n",
    "    print(\"-\" * 50)\n",
    "    auc_scores = [float(row[5]) for row in table_data]\n",
    "    print(f\"Average AUC: {sum(auc_scores)/len(auc_scores):.4f}\")\n",
    "    print(f\"Best AUC: {max(auc_scores):.4f} ({table_data[0][0]} model)\")\n",
    "\n",
    "    # Create and save visualization\n",
    "    fig = plot_comparison(results)\n",
    "    plt.savefig('performance_comparison.png')\n",
    "    plt.close()\n",
    "\n",
    "    print(\"\\nVisualization has been saved to 'performance_comparison.png'\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T11:17:04.586404Z",
     "iopub.status.busy": "2025-03-23T11:17:04.586186Z",
     "iopub.status.idle": "2025-03-23T11:17:04.614191Z",
     "shell.execute_reply": "2025-03-23T11:17:04.612538Z",
     "shell.execute_reply.started": "2025-03-23T11:17:04.586386Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "import json\n",
    "from train import preprocess_data\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def optimize_random_forest(trial, X, y):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 5, 30),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n",
    "        'class_weight': trial.suggest_categorical('class_weight', ['balanced', 'balanced_subsample', None]),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    model = RandomForestClassifier(**params)\n",
    "    return cross_val_score(model, X, y, cv=5, scoring='accuracy').mean()\n",
    "\n",
    "def optimize_svm(trial, X, y):\n",
    "    params = {\n",
    "        'C': trial.suggest_loguniform('C', 1e-3, 1e3),\n",
    "        'gamma': trial.suggest_loguniform('gamma', 1e-3, 1e3),\n",
    "        'kernel': trial.suggest_categorical('kernel', ['rbf', 'sigmoid', 'poly']),\n",
    "        'class_weight': trial.suggest_categorical('class_weight', ['balanced', None]),\n",
    "        'random_state': 42,\n",
    "        'probability': True\n",
    "    }\n",
    "    model = SVC(**params)\n",
    "    return cross_val_score(model, X, y, cv=5, scoring='accuracy').mean()\n",
    "\n",
    "def optimize_xgboost(trial, X, y):\n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.3),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 7),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n",
    "        'scale_pos_weight': trial.suggest_loguniform('scale_pos_weight', 1, 10),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    model = XGBClassifier(**params)\n",
    "    return cross_val_score(model, X, y, cv=5, scoring='accuracy').mean()\n",
    "\n",
    "def optimize_gbm(trial, X, y):\n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.1),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 15),\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n",
    "        'random_state': 42\n",
    "    }\n",
    "    model = GradientBoostingClassifier(**params)\n",
    "    return cross_val_score(model, X, y, cv=5, scoring='roc_auc').mean()\n",
    "\n",
    "def optimize_mlp(trial, X, y):\n",
    "    layers = trial.suggest_int('n_layers', 1, 3)\n",
    "    params = {\n",
    "        'hidden_layer_sizes': tuple(\n",
    "            trial.suggest_int(f'n_units_l{i}', 32, 256) for i in range(layers)\n",
    "        ),\n",
    "        'learning_rate_init': trial.suggest_loguniform('learning_rate_init', 1e-4, 1e-1),\n",
    "        'max_iter': 1000,\n",
    "        'random_state': 42\n",
    "    }\n",
    "    model = MLPClassifier(**params)\n",
    "    return cross_val_score(model, X, y, cv=5, scoring='roc_auc').mean()\n",
    "\n",
    "def optimize_knn(trial, X, y):\n",
    "    params = {\n",
    "        'n_neighbors': trial.suggest_int('n_neighbors', 1, 20),\n",
    "        'weights': trial.suggest_categorical('weights', ['uniform', 'distance']),\n",
    "        'p': trial.suggest_int('p', 1, 2)  # 1 for manhattan, 2 for euclidean\n",
    "    }\n",
    "    model = KNeighborsClassifier(**params)\n",
    "    return cross_val_score(model, X, y, cv=5, scoring='roc_auc').mean()\n",
    "\n",
    "def main():\n",
    "    # Load and preprocess data\n",
    "    data = pd.read_csv(\"/kaggle/input/glaucoma/glaucoma_dataset.csv\", skipinitialspace=True)\n",
    "    processed_data = preprocess_data(data)\n",
    "    X = processed_data.values\n",
    "    y = (data['Diagnosis'] == 'Glaucoma').astype(int).values\n",
    "\n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Dictionary of optimization functions\n",
    "    optimization_funcs = {\n",
    "        'Random_Forest': optimize_random_forest,\n",
    "        'SVM': optimize_svm,\n",
    "        'XGBoost': optimize_xgboost,\n",
    "        'GBM': optimize_gbm,\n",
    "        'Neural_Network': optimize_mlp,\n",
    "        'KNN': optimize_knn\n",
    "    }\n",
    "\n",
    "    # Store best parameters and scores\n",
    "    best_params = {}\n",
    "    best_scores = {}\n",
    "\n",
    "    # Run optimization for each model\n",
    "    n_trials = 100  # Increased from 50 to 100\n",
    "    for model_name, optimize_func in optimization_funcs.items():\n",
    "        print(f\"\\nOptimizing {model_name}...\")\n",
    "        study = optuna.create_study(direction='maximize')\n",
    "        study.optimize(lambda trial: optimize_func(trial, X_scaled, y), \n",
    "                      n_trials=n_trials)\n",
    "        \n",
    "        best_params[model_name] = study.best_params\n",
    "        best_scores[model_name] = study.best_value\n",
    "        \n",
    "        print(f\"Best {model_name} parameters:\", study.best_params)\n",
    "        print(f\"Best {model_name} score:\", study.best_value)\n",
    "\n",
    "    # Save results\n",
    "    results = {\n",
    "        'best_parameters': best_params,\n",
    "        'best_scores': best_scores\n",
    "    }\n",
    "    \n",
    "    with open('hyperparameter_optimization_results.json', 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "\n",
    "    print(\"\\nOptimization results saved to hyperparameter_optimization_results.json\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T11:17:25.506753Z",
     "iopub.status.busy": "2025-03-23T11:17:25.506434Z",
     "iopub.status.idle": "2025-03-23T11:17:25.629359Z",
     "shell.execute_reply": "2025-03-23T11:17:25.628523Z",
     "shell.execute_reply.started": "2025-03-23T11:17:25.50673Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "### logistic regression:\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "import joblib\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def load_best_params():\n",
    "    try:\n",
    "        with open('fast_hyperopt_results.json', 'r') as f:\n",
    "            return json.load(f)['best_parameters'].get('Logistic_Regression', {})\n",
    "    except FileNotFoundError:\n",
    "        try:\n",
    "            with open('hyperparameter_optimization_results.json', 'r') as f:\n",
    "                return json.load(f)['best_parameters'].get('Logistic_Regression', {})\n",
    "        except FileNotFoundError:\n",
    "            return {}\n",
    "\n",
    "def train(X_train, y_train):\n",
    "    # Load best parameters if available\n",
    "    best_params = load_best_params()\n",
    "    default_params = {'max_iter': 2000}\n",
    "    model_params = {**default_params, **best_params}\n",
    "    \n",
    "    # Create pipeline with SMOTE and model\n",
    "    pipeline = ImbPipeline([\n",
    "        ('sampling', SMOTE(random_state=42)),\n",
    "        ('model', LogisticRegression(**model_params))\n",
    "    ])\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\nTraining Logistic Regression...\")\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Save model\n",
    "    model_path = 'glaucoma_model_Logistic_Regression.joblib'\n",
    "    joblib.dump(pipeline, model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "def evaluate(model, X, y):\n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    # Get probability predictions if available\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_prob = model.predict_proba(X)[:, 1]\n",
    "        # Convert probability predictions to binary if needed\n",
    "        if np.any((y_pred > 1) | (y_pred < 0)):\n",
    "            y_pred = (y_prob >= 0.5).astype(int)\n",
    "    else:\n",
    "        y_prob = y_pred\n",
    "        y_pred = (y_pred >= 0.5).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y, y_pred),\n",
    "        'precision': precision_score(y, y_pred),\n",
    "        'recall': recall_score(y, y_pred),\n",
    "        'f1': f1_score(y, y_pred),\n",
    "        'auc': roc_auc_score(y, y_prob)\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def predict(model, X):\n",
    "    return model.predict(X)\n",
    "\n",
    "def predict_proba(model, X):\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        return model.predict_proba(X)\n",
    "    return model.predict(X)\n",
    "\n",
    "def load_model(filepath='glaucoma_model_Logistic_Regression.joblib'):\n",
    "    return joblib.load(filepath)\n",
    "\n",
    "def main():\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Load and preprocess data (simplified example)\n",
    "    # In a real application, you would use the same preprocessing as in train.py\n",
    "    data = pd.read_csv(\"/kaggle/input/glaucoma/glaucoma_dataset.csv\")\n",
    "    X = data.drop('Diagnosis', axis=1).select_dtypes(include=['number']).fillna(0)\n",
    "    y = (data['Diagnosis'] == 'Glaucoma').astype(int)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # Scale data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    \n",
    "    # Train model\n",
    "    model = train(X_train_scaled, y_train)\n",
    "    \n",
    "    # Evaluate model\n",
    "    metrics = evaluate(model, X_val_scaled, y_val)\n",
    "    print(\"\\nModel Performance:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T11:17:43.335948Z",
     "iopub.status.busy": "2025-03-23T11:17:43.33562Z",
     "iopub.status.idle": "2025-03-23T11:17:43.532095Z",
     "shell.execute_reply": "2025-03-23T11:17:43.531428Z",
     "shell.execute_reply.started": "2025-03-23T11:17:43.335921Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "### decision tree:\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "import joblib\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def load_best_params():\n",
    "    try:\n",
    "        with open('fast_hyperopt_results.json', 'r') as f:\n",
    "            return json.load(f)['best_parameters'].get('Decision_Tree', {})\n",
    "    except FileNotFoundError:\n",
    "        try:\n",
    "            with open('hyperparameter_optimization_results.json', 'r') as f:\n",
    "                return json.load(f)['best_parameters'].get('Decision_Tree', {})\n",
    "        except FileNotFoundError:\n",
    "            return {}\n",
    "\n",
    "def train(X_train, y_train):\n",
    "    # Load best parameters if available\n",
    "    best_params = load_best_params()\n",
    "    default_params = {'random_state': 42}\n",
    "    model_params = {**default_params, **best_params}\n",
    "    \n",
    "    # Create pipeline with SMOTE and model\n",
    "    pipeline = ImbPipeline([\n",
    "        ('sampling', SMOTE(random_state=42)),\n",
    "        ('model', DecisionTreeClassifier(**model_params))\n",
    "    ])\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\nTraining Decision Tree...\")\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Save model\n",
    "    model_path = 'glaucoma_model_Decision_Tree.joblib'\n",
    "    joblib.dump(pipeline, model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "def evaluate(model, X, y):\n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    # Get probability predictions if available\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_prob = model.predict_proba(X)[:, 1]\n",
    "        # Convert probability predictions to binary if needed\n",
    "        if np.any((y_pred > 1) | (y_pred < 0)):\n",
    "            y_pred = (y_prob >= 0.5).astype(int)\n",
    "    else:\n",
    "        y_prob = y_pred\n",
    "        y_pred = (y_pred >= 0.5).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y, y_pred),\n",
    "        'precision': precision_score(y, y_pred),\n",
    "        'recall': recall_score(y, y_pred),\n",
    "        'f1': f1_score(y, y_pred),\n",
    "        'auc': roc_auc_score(y, y_prob)\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def predict(model, X):\n",
    "    return model.predict(X)\n",
    "\n",
    "def predict_proba(model, X):\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        return model.predict_proba(X)\n",
    "    return model.predict(X)\n",
    "\n",
    "def load_model(filepath='glaucoma_model_Decision_Tree.joblib'):\n",
    "    return joblib.load(filepath)\n",
    "\n",
    "def main():\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Load and preprocess data (simplified example)\n",
    "    # In a real application, you would use the same preprocessing as in train.py\n",
    "    data = pd.read_csv(\"/kaggle/input/glaucoma/glaucoma_dataset.csv\")\n",
    "    X = data.drop('Diagnosis', axis=1).select_dtypes(include=['number']).fillna(0)\n",
    "    y = (data['Diagnosis'] == 'Glaucoma').astype(int)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # Scale data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    \n",
    "    # Train model\n",
    "    model = train(X_train_scaled, y_train)\n",
    "    \n",
    "    # Evaluate model\n",
    "    metrics = evaluate(model, X_val_scaled, y_val)\n",
    "    print(\"\\nModel Performance:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T11:17:52.6065Z",
     "iopub.status.busy": "2025-03-23T11:17:52.606169Z",
     "iopub.status.idle": "2025-03-23T11:17:54.794089Z",
     "shell.execute_reply": "2025-03-23T11:17:54.79334Z",
     "shell.execute_reply.started": "2025-03-23T11:17:52.606478Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "### random forest model:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T11:18:05.801634Z",
     "iopub.status.busy": "2025-03-23T11:18:05.801307Z",
     "iopub.status.idle": "2025-03-23T11:20:41.206139Z",
     "shell.execute_reply": "2025-03-23T11:20:41.20529Z",
     "shell.execute_reply.started": "2025-03-23T11:18:05.801608Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "###XAI for Random Forest(seconf best)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import shap\n",
    "import lime.lime_tabular\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.inspection import permutation_importance, PartialDependenceDisplay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# -------------------------------\n",
    "# ðŸ”¹ Load Data and Preprocess\n",
    "# -------------------------------\n",
    "data = pd.read_csv(\"/kaggle/input/glaucoma/glaucoma_dataset.csv\")\n",
    "X = data.drop('Diagnosis', axis=1).select_dtypes(include=['number']).fillna(0)\n",
    "y = (data['Diagnosis'] == 'Glaucoma').astype(int)\n",
    "\n",
    "# Convert DataFrame to NumPy array for model compatibility\n",
    "X_numpy = X.values\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_numpy, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale Data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Load Pretrained Random Forest Model\n",
    "rf_model = joblib.load(\"glaucoma_model_Random_Forest.joblib\")\n",
    "\n",
    "# Select a Sample for Explanation\n",
    "sample_idx = 10  # Pick a random test sample\n",
    "X_sample = X_test[sample_idx].reshape(1, -1)  # Convert single instance to NumPy array\n",
    "\n",
    "# -------------------------------\n",
    "# ðŸ”¹ 1. Feature Importance (Random Forest)\n",
    "# -------------------------------\n",
    "feature_importance = pd.Series(rf_model.named_steps['model'].feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=feature_importance[:10], y=feature_importance.index[:10], palette=\"coolwarm\")\n",
    "plt.xlabel(\"Feature Importance Score\")\n",
    "plt.title(\"Top 10 Feature Importance (Random Forest)\")\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# ðŸ”¹ 2. Partial Dependence Plot (PDP) - FIXED\n",
    "# -------------------------------\n",
    "# Use only features present in the dataset\n",
    "valid_features = [f for f in feature_importance.index if f in X.columns][:2]  # Pick first 2 valid features\n",
    "\n",
    "if valid_features:\n",
    "    display = PartialDependenceDisplay.from_estimator(rf_model.named_steps['model'], X_train_scaled, features=[X.columns.get_loc(f) for f in valid_features], grid_resolution=50)\n",
    "    display.plot()\n",
    "    plt.suptitle(\"Partial Dependence Plots (Top 2 Features)\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No valid features found for Partial Dependence Plot.\")\n",
    "\n",
    "# -------------------------------\n",
    "# ðŸ”¹ 3. Permutation Importance\n",
    "# -------------------------------\n",
    "perm_importance = permutation_importance(rf_model, X_test_scaled, y_test, scoring='accuracy', n_repeats=10, random_state=42)\n",
    "perm_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': perm_importance.importances_mean})\n",
    "perm_importance_df = perm_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=perm_importance_df[\"Importance\"][:10], y=perm_importance_df[\"Feature\"][:10], palette=\"coolwarm\")\n",
    "plt.xlabel(\"Permutation Importance Score\")\n",
    "plt.title(\"Top 10 Permutation Importance (Random Forest)\")\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# ðŸ”¹ 4. LIME Explanation (for one sample)\n",
    "# -------------------------------\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(X_train_scaled, feature_names=X.columns, class_names=['No Glaucoma', 'Glaucoma'], discretize_continuous=True)\n",
    "exp = explainer.explain_instance(X_sample[0], rf_model.predict_proba, num_features=5)\n",
    "exp.show_in_notebook()\n",
    "\n",
    "# -------------------------------\n",
    "# ðŸ”¹ 5. SHAP Explanation (Limited Visualizations)\n",
    "# -------------------------------\n",
    "explainer = shap.Explainer(rf_model.predict_proba, X_train_scaled)\n",
    "shap_values = explainer(X_test_scaled)\n",
    "\n",
    "# Summary Plot (Top 10 features only)\n",
    "shap.summary_plot(shap_values[..., 1], X_test_scaled, feature_names=X.columns, max_display=10)  # Class 1 (Glaucoma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T11:20:51.664639Z",
     "iopub.status.busy": "2025-03-23T11:20:51.664311Z",
     "iopub.status.idle": "2025-03-23T11:21:11.182511Z",
     "shell.execute_reply": "2025-03-23T11:21:11.181566Z",
     "shell.execute_reply.started": "2025-03-23T11:20:51.664614Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "### svm model:\n",
    "from sklearn.svm import SVC\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "import joblib\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def load_best_params():\n",
    "    try:\n",
    "        with open('fast_hyperopt_results.json', 'r') as f:\n",
    "            return json.load(f)['best_parameters'].get('SVM', {})\n",
    "    except FileNotFoundError:\n",
    "        try:\n",
    "            with open('hyperparameter_optimization_results.json', 'r') as f:\n",
    "                return json.load(f)['best_parameters'].get('SVM', {})\n",
    "        except FileNotFoundError:\n",
    "            return {}\n",
    "\n",
    "def train(X_train, y_train):\n",
    "    # Load best parameters if available\n",
    "    best_params = load_best_params()\n",
    "    default_params = {'random_state': 42, 'probability': True}\n",
    "    model_params = {**default_params, **best_params}\n",
    "    \n",
    "    # Create pipeline with SMOTE and model\n",
    "    pipeline = ImbPipeline([\n",
    "        ('sampling', SMOTE(random_state=42)),\n",
    "        ('model', SVC(**model_params))\n",
    "    ])\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\nTraining SVM...\")\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Save model\n",
    "    model_path = 'glaucoma_model_SVM.joblib'\n",
    "    joblib.dump(pipeline, model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "def evaluate(model, X, y):\n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    # Get probability predictions if available\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_prob = model.predict_proba(X)[:, 1]\n",
    "        # Convert probability predictions to binary if needed\n",
    "        if np.any((y_pred > 1) | (y_pred < 0)):\n",
    "            y_pred = (y_prob >= 0.5).astype(int)\n",
    "    else:\n",
    "        y_prob = y_pred\n",
    "        y_pred = (y_pred >= 0.5).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y, y_pred),\n",
    "        'precision': precision_score(y, y_pred),\n",
    "        'recall': recall_score(y, y_pred),\n",
    "        'f1': f1_score(y, y_pred),\n",
    "        'auc': roc_auc_score(y, y_prob)\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def predict(model, X):\n",
    "    return model.predict(X)\n",
    "\n",
    "def predict_proba(model, X):\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        return model.predict_proba(X)\n",
    "    return model.predict(X)\n",
    "\n",
    "def load_model(filepath='glaucoma_model_SVM.joblib'):\n",
    "    return joblib.load(filepath)\n",
    "\n",
    "def main():\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Load and preprocess data (simplified example)\n",
    "    # In a real application, you would use the same preprocessing as in train.py\n",
    "    data = pd.read_csv(\"/kaggle/input/glaucoma/glaucoma_dataset.csv\")\n",
    "    X = data.drop('Diagnosis', axis=1).select_dtypes(include=['number']).fillna(0)\n",
    "    y = (data['Diagnosis'] == 'Glaucoma').astype(int)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # Scale data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    \n",
    "    # Train model\n",
    "    model = train(X_train_scaled, y_train)\n",
    "    \n",
    "    # Evaluate model\n",
    "    metrics = evaluate(model, X_val_scaled, y_val)\n",
    "    print(\"\\nModel Performance:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T12:10:24.158893Z",
     "iopub.status.busy": "2025-03-23T12:10:24.158554Z",
     "iopub.status.idle": "2025-03-23T12:16:01.15296Z",
     "shell.execute_reply": "2025-03-23T12:16:01.151743Z",
     "shell.execute_reply.started": "2025-03-23T12:10:24.158871Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "###XAI on SVM (second best)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import shap\n",
    "import lime.lime_tabular\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.inspection import permutation_importance, PartialDependenceDisplay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# -------------------------------\n",
    "# ðŸ”¹ Load Data and Preprocess\n",
    "# -------------------------------\n",
    "data = pd.read_csv(\"/kaggle/input/glaucoma/glaucoma_dataset.csv\")\n",
    "X = data.drop('Diagnosis', axis=1).select_dtypes(include=['number']).fillna(0)\n",
    "y = (data['Diagnosis'] == 'Glaucoma').astype(int)\n",
    "\n",
    "# Convert DataFrame to NumPy array for model compatibility\n",
    "X_numpy = X.values\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_numpy, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale Data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Load Pretrained SVM Model\n",
    "svm_model = joblib.load(\"glaucoma_model_SVM.joblib\")\n",
    "\n",
    "# Select a Sample for Explanation\n",
    "sample_idx = 10  # Pick a random test sample\n",
    "X_sample = X_test[sample_idx].reshape(1, -1)  # Convert single instance to NumPy array\n",
    "\n",
    "# -------------------------------\n",
    "# ðŸ”¹ 1. Feature Importance via Permutation Importance (SVM has no built-in feature importance)\n",
    "# -------------------------------\n",
    "perm_importance = permutation_importance(svm_model, X_test_scaled, y_test, scoring='accuracy', n_repeats=10, random_state=42)\n",
    "perm_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': perm_importance.importances_mean})\n",
    "perm_importance_df = perm_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=perm_importance_df[\"Importance\"][:10], y=perm_importance_df[\"Feature\"][:10], palette=\"coolwarm\")\n",
    "plt.xlabel(\"Permutation Importance Score\")\n",
    "plt.title(\"Top 10 Feature Importance (SVM)\")\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# ðŸ”¹ 2. Partial Dependence Plot (PDP) - FIXED\n",
    "# -------------------------------\n",
    "# Use only features present in the dataset\n",
    "valid_features = perm_importance_df[\"Feature\"][:2].tolist()  # Pick first 2 valid features\n",
    "\n",
    "if valid_features:\n",
    "    display = PartialDependenceDisplay.from_estimator(svm_model, X_train_scaled, features=[X.columns.get_loc(f) for f in valid_features], grid_resolution=50)\n",
    "    display.plot()\n",
    "    plt.suptitle(\"Partial Dependence Plots (Top 2 Features)\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No valid features found for Partial Dependence Plot.\")\n",
    "\n",
    "# -------------------------------\n",
    "# ðŸ”¹ 3. Permutation Importance (Direct Interpretation)\n",
    "# -------------------------------\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=perm_importance_df[\"Importance\"][:10], y=perm_importance_df[\"Feature\"][:10], palette=\"coolwarm\")\n",
    "plt.xlabel(\"Permutation Importance Score\")\n",
    "plt.title(\"Top 10 Permutation Importance (SVM Model)\")\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# ðŸ”¹ 4. LIME Explanation (for one sample)\n",
    "# -------------------------------\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(X_train_scaled, feature_names=X.columns, class_names=['No Glaucoma', 'Glaucoma'], discretize_continuous=True)\n",
    "exp = explainer.explain_instance(X_sample[0], svm_model.decision_function, num_features=5)\n",
    "exp.show_in_notebook()\n",
    "\n",
    "# -------------------------------\n",
    "# ðŸ”¹ 5. SHAP Explanation (Limited Visualizations)\n",
    "# -------------------------------\n",
    "explainer = shap.Explainer(svm_model.decision_function, X_train_scaled)\n",
    "shap_values = explainer(X_test_scaled)\n",
    "\n",
    "# Summary Plot (Top 10 features only)\n",
    "shap.summary_plot(shap_values, X_test_scaled, feature_names=X.columns, max_display=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T11:21:46.725225Z",
     "iopub.status.busy": "2025-03-23T11:21:46.724892Z",
     "iopub.status.idle": "2025-03-23T11:21:46.963934Z",
     "shell.execute_reply": "2025-03-23T11:21:46.963088Z",
     "shell.execute_reply.started": "2025-03-23T11:21:46.725196Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "### knn model:\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "import joblib\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def load_best_params():\n",
    "    try:\n",
    "        with open('fast_hyperopt_results.json', 'r') as f:\n",
    "            return json.load(f)['best_parameters'].get('KNN', {})\n",
    "    except FileNotFoundError:\n",
    "        try:\n",
    "            with open('hyperparameter_optimization_results.json', 'r') as f:\n",
    "                return json.load(f)['best_parameters'].get('KNN', {})\n",
    "        except FileNotFoundError:\n",
    "            return {}\n",
    "\n",
    "def train(X_train, y_train):\n",
    "    # Load best parameters if available\n",
    "    best_params = load_best_params()\n",
    "    default_params = {'n_neighbors': 5}\n",
    "    model_params = {**default_params, **best_params}\n",
    "    \n",
    "    # Create pipeline with SMOTE and model\n",
    "    pipeline = ImbPipeline([\n",
    "        ('sampling', SMOTE(random_state=42)),\n",
    "        ('model', KNeighborsClassifier(**model_params))\n",
    "    ])\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\nTraining KNN...\")\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Save model\n",
    "    model_path = 'glaucoma_model_KNN.joblib'\n",
    "    joblib.dump(pipeline, model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "def evaluate(model, X, y):\n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    # Get probability predictions if available\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_prob = model.predict_proba(X)[:, 1]\n",
    "        # Convert probability predictions to binary if needed\n",
    "        if np.any((y_pred > 1) | (y_pred < 0)):\n",
    "            y_pred = (y_prob >= 0.5).astype(int)\n",
    "    else:\n",
    "        y_prob = y_pred\n",
    "        y_pred = (y_pred >= 0.5).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y, y_pred),\n",
    "        'precision': precision_score(y, y_pred),\n",
    "        'recall': recall_score(y, y_pred),\n",
    "        'f1': f1_score(y, y_pred),\n",
    "        'auc': roc_auc_score(y, y_prob)\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def predict(model, X):\n",
    "    return model.predict(X)\n",
    "\n",
    "def predict_proba(model, X):\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        return model.predict_proba(X)\n",
    "    return model.predict(X)\n",
    "\n",
    "def load_model(filepath='glaucoma_model_KNN.joblib'):\n",
    "    return joblib.load(filepath)\n",
    "\n",
    "def main():\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Load and preprocess data (simplified example)\n",
    "    # In a real application, you would use the same preprocessing as in train.py\n",
    "    data = pd.read_csv(\"/kaggle/input/glaucoma/glaucoma_dataset.csv\")\n",
    "    X = data.drop('Diagnosis', axis=1).select_dtypes(include=['number']).fillna(0)\n",
    "    y = (data['Diagnosis'] == 'Glaucoma').astype(int)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # Scale data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    \n",
    "    # Train model\n",
    "    model = train(X_train_scaled, y_train)\n",
    "    \n",
    "    # Evaluate model\n",
    "    metrics = evaluate(model, X_val_scaled, y_val)\n",
    "    print(\"\\nModel Performance:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T13:20:33.995487Z",
     "iopub.status.busy": "2025-03-23T13:20:33.995073Z",
     "iopub.status.idle": "2025-03-23T13:20:34.013316Z",
     "shell.execute_reply": "2025-03-23T13:20:34.012277Z",
     "shell.execute_reply.started": "2025-03-23T13:20:33.995452Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "knn_model = joblib.load(\"/content/glaucoma_model_KNN.joblib\")  # Update this path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T13:17:18.630852Z",
     "iopub.status.busy": "2025-03-23T13:17:18.630535Z",
     "iopub.status.idle": "2025-03-23T13:17:18.728707Z",
     "shell.execute_reply": "2025-03-23T13:17:18.727425Z",
     "shell.execute_reply.started": "2025-03-23T13:17:18.630831Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "### XAI on knn model(best performing model)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import shap\n",
    "\n",
    "import lime.lime_tabular\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance, PartialDependenceDisplay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# -------------------------------\n",
    "# ðŸ”¹ Load Data and Preprocess\n",
    "# -------------------------------\n",
    "data = pd.read_csv(\"/kaggle/input/glaucoma/glaucoma_dataset.csv\")\n",
    "X = data.drop('Diagnosis', axis=1).select_dtypes(include=['number']).fillna(0)\n",
    "y = (data['Diagnosis'] == 'Glaucoma').astype(int)\n",
    "\n",
    "# Convert DataFrame to NumPy array for KNN model\n",
    "X_numpy = X.values\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_numpy, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale Data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Load Pretrained KNN Model\n",
    "knn_model = joblib.load(\"glaucoma_model_KNN.joblib\")\n",
    "\n",
    "# Train a Random Forest for Feature Importance (since KNN lacks built-in feature importance)\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Select a Sample for Explanation\n",
    "sample_idx = 10  # Pick a random test sample\n",
    "X_sample = X_test[sample_idx].reshape(1, -1)  # Convert single instance to NumPy array\n",
    "\n",
    "# -------------------------------\n",
    "# ðŸ”¹ 1. Feature Importance (Random Forest)\n",
    "# -------------------------------\n",
    "feature_importance = pd.Series(rf_model.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=feature_importance[:10], y=feature_importance.index[:10], palette=\"coolwarm\")\n",
    "plt.xlabel(\"Feature Importance Score\")\n",
    "plt.title(\"Top 10 Feature Importance (Random Forest)\")\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# ðŸ”¹ 2. Partial Dependence Plot (PDP) - FIXED\n",
    "# -------------------------------\n",
    "# Use only features present in the scaled dataset\n",
    "valid_features = [f for f in feature_importance.index if f in X.columns][:2]  # Pick the first 2 valid features\n",
    "\n",
    "if valid_features:\n",
    "    display = PartialDependenceDisplay.from_estimator(rf_model, X_train_scaled, features=[X.columns.get_loc(f) for f in valid_features], grid_resolution=50)\n",
    "    display.plot()\n",
    "    plt.suptitle(\"Partial Dependence Plots (Top 2 Features)\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No valid features found for Partial Dependence Plot.\")\n",
    "\n",
    "# -------------------------------\n",
    "# ðŸ”¹ 3. Permutation Importance\n",
    "# -------------------------------\n",
    "perm_importance = permutation_importance(knn_model, X_test_scaled, y_test, scoring='accuracy', n_repeats=10, random_state=42)\n",
    "perm_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': perm_importance.importances_mean})\n",
    "perm_importance_df = perm_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=perm_importance_df[\"Importance\"][:10], y=perm_importance_df[\"Feature\"][:10], palette=\"coolwarm\")\n",
    "plt.xlabel(\"Permutation Importance Score\")\n",
    "plt.title(\"Top 10 Permutation Importance (KNN Model)\")\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# ðŸ”¹ 4. LIME Explanation (for one sample)\n",
    "# -------------------------------\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(X_train_scaled, feature_names=X.columns, class_names=['No Glaucoma', 'Glaucoma'], discretize_continuous=True)\n",
    "exp = explainer.explain_instance(X_sample[0], knn_model.predict_proba, num_features=5)\n",
    "exp.show_in_notebook()\n",
    "\n",
    "# -------------------------------\n",
    "# ðŸ”¹ 5. SHAP Explanation (Limited Visualizations)\n",
    "# -------------------------------\n",
    "explainer = shap.Explainer(knn_model.predict_proba, X_train_scaled)\n",
    "shap_values = explainer(X_test_scaled)\n",
    "\n",
    "# Summary Plot (Top 10 features only)\n",
    "shap.summary_plot(shap_values[..., 1], X_test_scaled, feature_names=X.columns, max_display=10)  # Class 1 (Glaucoma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T11:24:20.489869Z",
     "iopub.status.busy": "2025-03-23T11:24:20.489526Z",
     "iopub.status.idle": "2025-03-23T11:24:21.640132Z",
     "shell.execute_reply": "2025-03-23T11:24:21.639217Z",
     "shell.execute_reply.started": "2025-03-23T11:24:20.489845Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "### gbm model:\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "import joblib\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def load_best_params():\n",
    "    try:\n",
    "        with open('fast_hyperopt_results.json', 'r') as f:\n",
    "            return json.load(f)['best_parameters'].get('GBM', {})\n",
    "    except FileNotFoundError:\n",
    "        try:\n",
    "            with open('hyperparameter_optimization_results.json', 'r') as f:\n",
    "                return json.load(f)['best_parameters'].get('GBM', {})\n",
    "        except FileNotFoundError:\n",
    "            return {}\n",
    "\n",
    "def train(X_train, y_train):\n",
    "    # Load best parameters if available\n",
    "    best_params = load_best_params()\n",
    "    default_params = {'random_state': 42}\n",
    "    model_params = {**default_params, **best_params}\n",
    "    \n",
    "    # Create pipeline with SMOTE and model\n",
    "    pipeline = ImbPipeline([\n",
    "        ('sampling', SMOTE(random_state=42)),\n",
    "        ('model', GradientBoostingClassifier(**model_params))\n",
    "    ])\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\nTraining Gradient Boosting...\")\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Save model\n",
    "    model_path = 'glaucoma_model_GBM.joblib'\n",
    "    joblib.dump(pipeline, model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "def evaluate(model, X, y):\n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    # Get probability predictions if available\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_prob = model.predict_proba(X)[:, 1]\n",
    "        # Convert probability predictions to binary if needed\n",
    "        if np.any((y_pred > 1) | (y_pred < 0)):\n",
    "            y_pred = (y_prob >= 0.5).astype(int)\n",
    "    else:\n",
    "        y_prob = y_pred\n",
    "        y_pred = (y_pred >= 0.5).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y, y_pred),\n",
    "        'precision': precision_score(y, y_pred),\n",
    "        'recall': recall_score(y, y_pred),\n",
    "        'f1': f1_score(y, y_pred),\n",
    "        'auc': roc_auc_score(y, y_prob)\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def predict(model, X):\n",
    "    return model.predict(X)\n",
    "\n",
    "def predict_proba(model, X):\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        return model.predict_proba(X)\n",
    "    return model.predict(X)\n",
    "\n",
    "def load_model(filepath='glaucoma_model_GBM.joblib'):\n",
    "    return joblib.load(filepath)\n",
    "\n",
    "def main():\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Load and preprocess data (simplified example)\n",
    "    # In a real application, you would use the same preprocessing as in train.py\n",
    "    data = pd.read_csv(\"/kaggle/input/glaucoma/glaucoma_dataset.csv\")\n",
    "    X = data.drop('Diagnosis', axis=1).select_dtypes(include=['number']).fillna(0)\n",
    "    y = (data['Diagnosis'] == 'Glaucoma').astype(int)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # Scale data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    \n",
    "    # Train model\n",
    "    model = train(X_train_scaled, y_train)\n",
    "    \n",
    "    # Evaluate model\n",
    "    metrics = evaluate(model, X_val_scaled, y_val)\n",
    "    print(\"\\nModel Performance:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T11:24:29.280002Z",
     "iopub.status.busy": "2025-03-23T11:24:29.279699Z",
     "iopub.status.idle": "2025-03-23T11:24:38.366363Z",
     "shell.execute_reply": "2025-03-23T11:24:38.365551Z",
     "shell.execute_reply.started": "2025-03-23T11:24:29.279979Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "###XAI on GBM (worst performing model)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import shap\n",
    "import lime.lime_tabular\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.inspection import permutation_importance, PartialDependenceDisplay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# -------------------------------\n",
    "# ðŸ”¹ Load Data and Preprocess\n",
    "# -------------------------------\n",
    "data = pd.read_csv(\"/kaggle/input/glaucoma/glaucoma_dataset.csv\")\n",
    "X = data.drop('Diagnosis', axis=1).select_dtypes(include=['number']).fillna(0)\n",
    "y = (data['Diagnosis'] == 'Glaucoma').astype(int)\n",
    "\n",
    "# Convert DataFrame to NumPy array for model compatibility\n",
    "X_numpy = X.values\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_numpy, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale Data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Load Pretrained GBM Model\n",
    "gbm_model = joblib.load(\"glaucoma_model_GBM.joblib\")\n",
    "\n",
    "# Select a Sample for Explanation\n",
    "sample_idx = 10  # Pick a random test sample\n",
    "X_sample = X_test[sample_idx].reshape(1, -1)  # Convert single instance to NumPy array\n",
    "\n",
    "# -------------------------------\n",
    "# ðŸ”¹ 1. Feature Importance (GBM)\n",
    "# -------------------------------\n",
    "feature_importance = pd.Series(gbm_model.named_steps['model'].feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=feature_importance[:10], y=feature_importance.index[:10], palette=\"coolwarm\")\n",
    "plt.xlabel(\"Feature Importance Score\")\n",
    "plt.title(\"Top 10 Feature Importance (GBM)\")\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# ðŸ”¹ 2. Partial Dependence Plot (PDP) - FIXED\n",
    "# -------------------------------\n",
    "# Use only features present in the dataset\n",
    "valid_features = [f for f in feature_importance.index if f in X.columns][:2]  # Pick first 2 valid features\n",
    "\n",
    "if valid_features:\n",
    "    display = PartialDependenceDisplay.from_estimator(gbm_model.named_steps['model'], X_train_scaled, features=[X.columns.get_loc(f) for f in valid_features], grid_resolution=50)\n",
    "    display.plot()\n",
    "    plt.suptitle(\"Partial Dependence Plots (Top 2 Features)\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No valid features found for Partial Dependence Plot.\")\n",
    "\n",
    "# -------------------------------\n",
    "# ðŸ”¹ 3. Permutation Importance\n",
    "# -------------------------------\n",
    "perm_importance = permutation_importance(gbm_model, X_test_scaled, y_test, scoring='accuracy', n_repeats=10, random_state=42)\n",
    "perm_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': perm_importance.importances_mean})\n",
    "perm_importance_df = perm_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=perm_importance_df[\"Importance\"][:10], y=perm_importance_df[\"Feature\"][:10], palette=\"coolwarm\")\n",
    "plt.xlabel(\"Permutation Importance Score\")\n",
    "plt.title(\"Top 10 Permutation Importance (GBM)\")\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# ðŸ”¹ 4. LIME Explanation (for one sample)\n",
    "# -------------------------------\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(X_train_scaled, feature_names=X.columns, class_names=['No Glaucoma', 'Glaucoma'], discretize_continuous=True)\n",
    "exp = explainer.explain_instance(X_sample[0], gbm_model.predict_proba, num_features=5)\n",
    "exp.show_in_notebook()\n",
    "\n",
    "# -------------------------------\n",
    "# ðŸ”¹ 5. SHAP Explanation (Limited Visualizations)\n",
    "# -------------------------------\n",
    "explainer = shap.Explainer(gbm_model.predict_proba, X_train_scaled)\n",
    "shap_values = explainer(X_test_scaled)\n",
    "\n",
    "# Summary Plot (Top 10 features only)\n",
    "shap.summary_plot(shap_values[..., 1], X_test_scaled, feature_names=X.columns, max_display=10)  # Class 1 (Glaucoma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T11:28:11.712186Z",
     "iopub.status.busy": "2025-03-23T11:28:11.711851Z",
     "iopub.status.idle": "2025-03-23T11:28:11.943851Z",
     "shell.execute_reply": "2025-03-23T11:28:11.942933Z",
     "shell.execute_reply.started": "2025-03-23T11:28:11.712162Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "### xgboost model:\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "import joblib\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def load_best_params():\n",
    "    try:\n",
    "        with open('fast_hyperopt_results.json', 'r') as f:\n",
    "            return json.load(f)['best_parameters'].get('XGBoost', {})\n",
    "    except FileNotFoundError:\n",
    "        try:\n",
    "            with open('hyperparameter_optimization_results.json', 'r') as f:\n",
    "                return json.load(f)['best_parameters'].get('XGBoost', {})\n",
    "        except FileNotFoundError:\n",
    "            return {}\n",
    "\n",
    "def train(X_train, y_train):\n",
    "    # Load best parameters if available\n",
    "    best_params = load_best_params()\n",
    "    default_params = {\n",
    "        'random_state': 42,\n",
    "        'objective': 'binary:logistic',\n",
    "        'use_label_encoder': False,\n",
    "        'eval_metric': 'logloss'\n",
    "    }\n",
    "    model_params = {**default_params, **best_params}\n",
    "    \n",
    "    # Create pipeline with SMOTE and model\n",
    "    pipeline = ImbPipeline([\n",
    "        ('sampling', SMOTE(random_state=42)),\n",
    "        ('model', XGBClassifier(**model_params))\n",
    "    ])\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\nTraining XGBoost...\")\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Save model\n",
    "    model_path = 'glaucoma_model_XGBoost.joblib'\n",
    "    joblib.dump(pipeline, model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "def evaluate(model, X, y):\n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    # Get probability predictions if available\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_prob = model.predict_proba(X)[:, 1]\n",
    "        # Convert probability predictions to binary if needed\n",
    "        if np.any((y_pred > 1) | (y_pred < 0)):\n",
    "            y_pred = (y_prob >= 0.5).astype(int)\n",
    "    else:\n",
    "        y_prob = y_pred\n",
    "        y_pred = (y_pred >= 0.5).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y, y_pred),\n",
    "        'precision': precision_score(y, y_pred),\n",
    "        'recall': recall_score(y, y_pred),\n",
    "        'f1': f1_score(y, y_pred),\n",
    "        'auc': roc_auc_score(y, y_prob)\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def predict(model, X):\n",
    "    return model.predict(X)\n",
    "\n",
    "def predict_proba(model, X):\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        return model.predict_proba(X)\n",
    "    return model.predict(X)\n",
    "\n",
    "def load_model(filepath='glaucoma_model_XGBoost.joblib'):\n",
    "    return joblib.load(filepath)\n",
    "\n",
    "def main():\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Load and preprocess data (simplified example)\n",
    "    # In a real application, you would use the same preprocessing as in train.py\n",
    "    data = pd.read_csv(\"/kaggle/input/glaucoma/glaucoma_dataset.csv\")\n",
    "    X = data.drop('Diagnosis', axis=1).select_dtypes(include=['number']).fillna(0)\n",
    "    y = (data['Diagnosis'] == 'Glaucoma').astype(int)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # Scale data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    \n",
    "    # Train model\n",
    "    model = train(X_train_scaled, y_train)\n",
    "    \n",
    "    # Evaluate model\n",
    "    metrics = evaluate(model, X_val_scaled, y_val)\n",
    "    print(\"\\nModel Performance:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-03-23T12:55:29.419Z",
     "iopub.execute_input": "2025-03-23T11:35:35.428057Z",
     "iopub.status.busy": "2025-03-23T11:35:35.427752Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "### XAI on XGBoost model (second worst performing medel)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import shap\n",
    "import lime.lime_tabular\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.inspection import permutation_importance, PartialDependenceDisplay\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# -------------------------------\n",
    "# ðŸ”¹ Load Data and Preprocess\n",
    "# -------------------------------\n",
    "data = pd.read_csv(\"/kaggle/input/glaucoma/glaucoma_dataset.csv\")\n",
    "X = data.drop('Diagnosis', axis=1).select_dtypes(include=['number']).fillna(0)\n",
    "y = (data['Diagnosis'] == 'Glaucoma').astype(int)\n",
    "\n",
    "# Convert DataFrame to NumPy array for model compatibility\n",
    "X_numpy = X.values\n",
    "\n",
    "# Split Data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_numpy, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Scale Data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Load Pretrained XGBoost Model\n",
    "xgb_model = joblib.load(\"glaucoma_model_XGBoost.joblib\")\n",
    "\n",
    "# Select a Sample for Explanation\n",
    "sample_idx = 10  # Pick a random test sample\n",
    "X_sample = X_test[sample_idx].reshape(1, -1)  # Convert single instance to NumPy array\n",
    "\n",
    "# -------------------------------\n",
    "# ðŸ”¹ 1. Feature Importance (XGBoost)\n",
    "# -------------------------------\n",
    "feature_importance = pd.Series(xgb_model.named_steps['model'].feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=feature_importance[:10], y=feature_importance.index[:10], palette=\"coolwarm\")\n",
    "plt.xlabel(\"Feature Importance Score\")\n",
    "plt.title(\"Top 10 Feature Importance (XGBoost)\")\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# ðŸ”¹ 2. Partial Dependence Plot (PDP) - FIXED\n",
    "# -------------------------------\n",
    "# Use only features present in the dataset\n",
    "valid_features = [f for f in feature_importance.index if f in X.columns][:2]  # Pick first 2 valid features\n",
    "\n",
    "if valid_features:\n",
    "    display = PartialDependenceDisplay.from_estimator(xgb_model.named_steps['model'], X_train_scaled, features=[X.columns.get_loc(f) for f in valid_features], grid_resolution=50)\n",
    "    display.plot()\n",
    "    plt.suptitle(\"Partial Dependence Plots (Top 2 Features)\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No valid features found for Partial Dependence Plot.\")\n",
    "\n",
    "# -------------------------------\n",
    "# ðŸ”¹ 3. Permutation Importance\n",
    "# -------------------------------\n",
    "perm_importance = permutation_importance(xgb_model, X_test_scaled, y_test, scoring='accuracy', n_repeats=10, random_state=42)\n",
    "perm_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': perm_importance.importances_mean})\n",
    "perm_importance_df = perm_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.barplot(x=perm_importance_df[\"Importance\"][:10], y=perm_importance_df[\"Feature\"][:10], palette=\"coolwarm\")\n",
    "plt.xlabel(\"Permutation Importance Score\")\n",
    "plt.title(\"Top 10 Permutation Importance (XGBoost)\")\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# ðŸ”¹ 4. LIME Explanation (for one sample)\n",
    "# -------------------------------\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(X_train_scaled, feature_names=X.columns, class_names=['No Glaucoma', 'Glaucoma'], discretize_continuous=True)\n",
    "exp = explainer.explain_instance(X_sample[0], xgb_model.predict_proba, num_features=5)\n",
    "exp.show_in_notebook()\n",
    "\n",
    "# -------------------------------\n",
    "# ðŸ”¹ 5. SHAP Explanation (Limited Visualizations)\n",
    "# -------------------------------\n",
    "explainer = shap.Explainer(xgb_model.predict_proba, X_train_scaled)\n",
    "shap_values = explainer(X_test_scaled)\n",
    "\n",
    "# Summary Plot (Top 10 features only)\n",
    "shap.summary_plot(shap_values[..., 1], X_test_scaled, feature_names=X.columns, max_display=10)  # Class 1 (Glaucoma)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-23T11:41:55.190751Z",
     "iopub.status.busy": "2025-03-23T11:41:55.190362Z",
     "iopub.status.idle": "2025-03-23T11:42:06.4047Z",
     "shell.execute_reply": "2025-03-23T11:42:06.403753Z",
     "shell.execute_reply.started": "2025-03-23T11:41:55.19072Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "### neural network model:\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\n",
    "import joblib\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "def load_best_params():\n",
    "    try:\n",
    "        with open('fast_hyperopt_results.json', 'r') as f:\n",
    "            return json.load(f)['best_parameters'].get('Neural_Network', {})\n",
    "    except FileNotFoundError:\n",
    "        try:\n",
    "            with open('hyperparameter_optimization_results.json', 'r') as f:\n",
    "                return json.load(f)['best_parameters'].get('Neural_Network', {})\n",
    "        except FileNotFoundError:\n",
    "            return {}\n",
    "\n",
    "def train(X_train, y_train):\n",
    "    # Load best parameters if available\n",
    "    best_params = load_best_params()\n",
    "    default_params = {\n",
    "        'hidden_layer_sizes': (64, 32),\n",
    "        'max_iter': 1000,\n",
    "        'random_state': 42\n",
    "    }\n",
    "    model_params = {**default_params, **best_params}\n",
    "    \n",
    "    # Create pipeline with SMOTE and model\n",
    "    pipeline = ImbPipeline([\n",
    "        ('sampling', SMOTE(random_state=42)),\n",
    "        ('model', MLPClassifier(**model_params))\n",
    "    ])\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\nTraining Neural Network...\")\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Save model\n",
    "    model_path = 'glaucoma_model_Neural_Network.joblib'\n",
    "    joblib.dump(pipeline, model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "def evaluate(model, X, y):\n",
    "    # Get predictions\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    # Get probability predictions if available\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        y_prob = model.predict_proba(X)[:, 1]\n",
    "        # Convert probability predictions to binary if needed\n",
    "        if np.any((y_pred > 1) | (y_pred < 0)):\n",
    "            y_pred = (y_prob >= 0.5).astype(int)\n",
    "    else:\n",
    "        y_prob = y_pred\n",
    "        y_pred = (y_pred >= 0.5).astype(int)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(y, y_pred),\n",
    "        'precision': precision_score(y, y_pred),\n",
    "        'recall': recall_score(y, y_pred),\n",
    "        'f1': f1_score(y, y_pred),\n",
    "        'auc': roc_auc_score(y, y_prob)\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def predict(model, X):\n",
    "    return model.predict(X)\n",
    "\n",
    "def predict_proba(model, X):\n",
    "    if hasattr(model, 'predict_proba'):\n",
    "        return model.predict_proba(X)\n",
    "    return model.predict(X)\n",
    "\n",
    "def load_model(filepath='glaucoma_model_Neural_Network.joblib'):\n",
    "    return joblib.load(filepath)\n",
    "\n",
    "def main():\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Load and preprocess data (simplified example)\n",
    "    # In a real application, you would use the same preprocessing as in train.py\n",
    "    data = pd.read_csv(\"/kaggle/input/glaucoma/glaucoma_dataset.csv\")\n",
    "    X = data.drop('Diagnosis', axis=1).select_dtypes(include=['number']).fillna(0)\n",
    "    y = (data['Diagnosis'] == 'Glaucoma').astype(int)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # Scale data\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    \n",
    "    # Train model\n",
    "    model = train(X_train_scaled, y_train)\n",
    "    \n",
    "    # Evaluate model\n",
    "    metrics = evaluate(model, X_val_scaled, y_val)\n",
    "    print(\"\\nModel Performance:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-03-23T11:17:04.629427Z",
     "iopub.status.idle": "2025-03-23T11:17:04.629807Z",
     "shell.execute_reply": "2025-03-23T11:17:04.629642Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 6733859,
     "sourceId": 10842945,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
