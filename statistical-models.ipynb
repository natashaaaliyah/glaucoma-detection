{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10842945,"sourceType":"datasetVersion","datasetId":6733859}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from sklearn.neural_network import MLPClassifier\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\nimport json\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.pipeline import Pipeline\nimport joblib\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.decomposition import PCA\nfrom xgboost import XGBClassifier\nfrom sklearn.base import clone  # Import clone from sklearn\n\nclass GlaucomaDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.FloatTensor(X)\n        self.y = torch.LongTensor(y)\n        \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\nclass Autoencoder(nn.Module):\n    def __init__(self, input_size, encoding_dim=64):\n        super(Autoencoder, self).__init__()\n        \n        # Encoder layers\n        self.encoder = nn.Sequential(\n            nn.Linear(input_size, 256),\n            nn.ReLU(),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, encoding_dim),\n            nn.ReLU()\n        )\n        \n        # Decoder layers\n        self.decoder = nn.Sequential(\n            nn.Linear(encoding_dim, 128),\n            nn.ReLU(),\n            nn.Linear(128, 256),\n            nn.ReLU(),\n            nn.Linear(256, input_size),\n            nn.Sigmoid()  # Output should be in the range [0, 1]\n        )\n\n    def forward(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return decoded\n\ndef train_autoencoder(model, train_loader, optimizer, criterion, device, num_epochs=50):\n    model.train()\n    for epoch in range(num_epochs):\n        train_loss = 0\n        for batch_X, _ in train_loader:\n            batch_X = batch_X.to(device)\n            \n            # Forward pass\n            outputs = model(batch_X)\n            loss = criterion(outputs, batch_X)\n            \n            # Backward and optimize\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item()\n        \n        print(f'Epoch [{epoch+1}/{num_epochs}], Autoencoder Loss: {train_loss/len(train_loader):.4f}')\n\ndef train_and_evaluate_models(X_train, X_val, y_train, y_val, scaler_mean, scaler_scale):\n    # First apply PCA\n    pca = PCA(n_components=0.95)  # Keep 95% of variance\n    X_train_pca = pca.fit_transform(X_train)\n    X_val_pca = pca.transform(X_val)\n    \n    results = {}\n    models = {\n        # 'Linear_Regression': Pipeline([\n        #     ('model', LinearRegression())\n        # ]),\n        'Logistic_Regression': Pipeline([\n            ('model', LogisticRegression(max_iter=2000))\n        ]),\n        'Decision_Tree': Pipeline([\n            ('model', DecisionTreeClassifier(random_state=42))\n        ]),\n        'Random_Forest': Pipeline([\n            ('model', RandomForestClassifier(n_estimators=100, random_state=42))\n        ]),\n        'SVM': Pipeline([\n            ('model', SVC(probability=True, kernel='rbf', random_state=42))\n        ]),\n        'KNN': Pipeline([\n            ('model', KNeighborsClassifier(n_neighbors=5))\n        ]),\n        'GBM': Pipeline([\n            ('model', GradientBoostingClassifier(random_state=42))\n        ]),\n        'XGBoost': Pipeline([\n            ('model', XGBClassifier(random_state=42,\n                                     learning_rate=0.1,\n                                     n_estimators=100,\n                                     max_depth=5))\n        ]),\n        'Neural_Network': Pipeline([\n            ('model', MLPClassifier(hidden_layer_sizes=(64, 32),\n                                    max_iter=1000,\n                                    random_state=42))\n        ])\n    }\n\n    # Train and evaluate each model on both original and PCA features\n    for name, model in models.items():\n        print(f\"\\nTraining {name}...\")\n        \n        # Train on original features\n        model.fit(X_train, y_train)\n        metrics_orig = evaluate_model(model, X_val, y_val)\n        results[f'{name}_original'] = metrics_orig\n        joblib.dump(model, f'glaucoma_model_{name}_original.joblib')\n        \n        # Train on PCA-reduced features using sklearn's clone\n        model_pca = clone(model)\n        model_pca.fit(X_train_pca, y_train)\n        metrics_pca = evaluate_model(model_pca, X_val_pca, y_val)\n        results[f'{name}_pca'] = metrics_pca\n        joblib.dump((model_pca, pca), f'glaucoma_model_{name}_pca.joblib')\n    \n    return results\n\ndef evaluate_model(model, X, y):\n    \"\"\"Helper function to evaluate a model\"\"\"\n    y_pred = model.predict(X)\n    y_prob = model.predict_proba(X)[:, 1] if hasattr(model, 'predict_proba') else y_pred\n    \n    return {\n        'accuracy': accuracy_score(y, y_pred),\n        'precision': precision_score(y, y_pred),\n        'recall': recall_score(y, y_pred),\n        'f1': f1_score(y, y_pred),\n        'auc': roc_auc_score(y, y_prob)\n    }\n\ndef preprocess_data(data):\n    def extract_number(value):\n        if pd.isna(value):\n            return np.nan\n        # Extract the first number found in the string\n        import re\n        numbers = re.findall(r'[-+]?\\d*\\.\\d+|\\d+', str(value))\n        return float(numbers[0]) if numbers else np.nan\n    \n    # Select and preprocess numerical features\n    numerical_features = {\n        'Age': 'Age',\n        'Visual Acuity': 'Visual Acuity Measurements',\n        'IOP': 'Intraocular Pressure (IOP)',\n        'CDR': 'Cup-to-Disc Ratio (CDR)',\n        'RNFL': 'Optical Coherence Tomography (OCT) Results',\n        'Pachymetry': 'Pachymetry'\n    }\n    \n    processed_data = pd.DataFrame()\n    for new_name, original_name in numerical_features.items():\n        processed_data[new_name] = data[original_name].apply(extract_number)\n    \n    # Handle Visual Field Test Results separately since it has sensitivity/specificity\n    vf_data = data['Visual Field Test Results'].str.extract(r'Sensitivity: ([\\d.]+), Specificity: ([\\d.]+)')\n    processed_data['VF_Sensitivity'] = pd.to_numeric(vf_data[0])\n    processed_data['VF_Specificity'] = pd.to_numeric(vf_data[1])\n    \n    # Handle OCT measurements\n    oct_data = data['Optical Coherence Tomography (OCT) Results'].str.extract(\n        r'RNFL Thickness: ([\\d.]+).*GCC Thickness: ([\\d.]+).*Retinal Volume: ([\\d.]+).*Macular Thickness: ([\\d.]+)'\n    )\n    processed_data['RNFL_Thickness'] = pd.to_numeric(oct_data[0])\n    processed_data['GCC_Thickness'] = pd.to_numeric(oct_data[1])\n    processed_data['Retinal_Volume'] = pd.to_numeric(oct_data[2])\n    processed_data['Macular_Thickness'] = pd.to_numeric(oct_data[3])\n    \n    return processed_data\n\ndef main():\n    # Data preparation\n    data = pd.read_csv(\"/kaggle/input/glaucoma/glaucoma_dataset.csv\", skipinitialspace=True)\n    processed_data = preprocess_data(data)\n    X = processed_data.values\n    y = (data['Diagnosis'] == 'Glaucoma').astype(int).values\n    \n    # Split data\n    X_train, X_val, y_train, y_val = train_test_split(\n        X, y, test_size=0.2, random_state=42, stratify=y\n    )\n    \n    # Scale data\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_val_scaled = scaler.transform(X_val)\n    \n    # Save scaler parameters\n    scaler_mean = scaler.mean_\n    scaler_scale = scaler.scale_\n    \n    # Create datasets and dataloaders\n    train_dataset = GlaucomaDataset(X_train_scaled, y_train)\n    val_dataset = GlaucomaDataset(X_val_scaled, y_val)\n    \n    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n    \n    # Train autoencoder\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    autoencoder = Autoencoder(input_size=X.shape[1]).to(device)\n    ae_optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n    ae_criterion = nn.MSELoss()\n    \n    print(\"\\nTraining Autoencoder...\")\n    train_autoencoder(autoencoder, train_loader, ae_optimizer, ae_criterion, device, num_epochs=50)\n    \n    # Extract features from autoencoder\n    with torch.no_grad():\n        X_train_encoded = autoencoder.encoder(torch.FloatTensor(X_train_scaled).to(device)).cpu().numpy()\n        X_val_encoded = autoencoder.encoder(torch.FloatTensor(X_val_scaled).to(device)).cpu().numpy()\n    \n    # Train classical ML models\n    print(\"\\nTraining models on original features...\")\n    ml_results = train_and_evaluate_models(X_train_scaled, X_val_scaled, y_train, y_val, scaler_mean, scaler_scale)\n    \n    print(\"\\nTraining models on autoencoder features...\")\n    ml_results_encoded = train_and_evaluate_models(X_train_encoded, X_val_encoded, y_train, y_val, scaler_mean, scaler_scale)\n    \n    # Combine results\n    ml_results.update({f\"{k}_Encoded\": v for k, v in ml_results_encoded.items()})\n    \n    # Print and save all results\n    print(\"\\nModel Performance Summary:\")\n    print(\"=\" * 50)\n    for model_name, metrics in ml_results.items():\n        print(f\"\\n{model_name}:\")\n        for metric, value in metrics.items():\n            print(f\"{metric}: {value:.4f}\")\n    \n    # Save metrics to file\n    with open('model_metrics.json', 'w') as f:\n        json.dump(ml_results, f, indent=4)\n    \n    print(\"\\nMetrics saved to model_metrics.json\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-23T11:13:15.321694Z","iopub.execute_input":"2025-03-23T11:13:15.322009Z","iopub.status.idle":"2025-03-23T11:17:03.81531Z","shell.execute_reply.started":"2025-03-23T11:13:15.321986Z","shell.execute_reply":"2025-03-23T11:17:03.814358Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom tabulate import tabulate\nimport json\n\ndef display_metrics_in_terminal(json_path):\n    with open(json_path, 'r') as f:\n        data = json.load(f)\n    \n    table_data = []\n    for model_name, metrics in data.items():\n        table_data.append([\n            model_name,\n            metrics['accuracy'],\n            metrics['precision'],\n            metrics['recall'],\n            metrics['f1'],\n            metrics['auc']\n        ])\n    \n    headers = [\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"AUC\"]\n    print(tabulate(table_data, headers=headers, tablefmt='psql'))\n\nif __name__ == \"__main__\":\n    display_metrics_in_terminal(\"model_metrics.json\")\n    \nimport json\nfrom tabulate import tabulate\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef load_results(filepath):\n    with open(filepath, 'r') as f:\n        return json.load(f)\n\ndef create_performance_table(results):\n    # Prepare data for tabulation\n    table_data = []\n    headers = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1', 'AUC']\n\n    for model_name, model_data in results.items():\n        row = [\n            model_name.upper(),\n            f\"{model_data['accuracy']:.4f}\",\n            f\"{model_data['precision']:.4f}\",\n            f\"{model_data['recall']:.4f}\",\n            f\"{model_data['f1']:.4f}\",\n            f\"{model_data['auc']:.4f}\"\n        ]\n        table_data.append(row)\n\n    # Sort by AUC\n    table_data.sort(key=lambda x: float(x[5]), reverse=True)\n    return table_data, headers\n\ndef plot_comparison(results):\n    models = list(results.keys())\n    auc_scores = [results[m]['auc'] for m in models]\n    f1_scores = [results[m]['f1'] for m in models]\n\n    # Create figure with subplots\n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 10))\n\n    # Bar plot comparing AUC scores\n    ax1.bar(models, auc_scores, label='AUC', color='skyblue')\n    ax1.set_ylabel('AUC Score')\n    ax1.set_title('AUC Scores by Model')\n    ax1.set_xticks(range(len(models)))\n    ax1.set_xticklabels([m.upper() for m in models], rotation=45)\n    ax1.legend()\n    ax1.grid(True, alpha=0.3)\n\n    # Bar plot showing F1 scores\n    ax2.bar(models, f1_scores, color='lightgreen')\n    ax2.set_ylabel('F1 Score')\n    ax2.set_title('F1 Scores by Model')\n    ax2.set_xticks(range(len(models)))\n    ax2.set_xticklabels([m.upper() for m in models], rotation=45)\n    ax2.grid(True, alpha=0.3)\n\n    plt.tight_layout()\n    return fig\n\ndef main():\n    # Load results\n    results = load_results('model_metrics.json')\n\n    # Create and display performance table\n    table_data, headers = create_performance_table(results)\n    print(\"\\nModel Performance Summary:\")\n    print(tabulate(table_data, headers=headers, tablefmt='grid'))\n\n    # Print summary statistics\n    print(\"\\nSummary Statistics:\")\n    print(\"-\" * 50)\n    auc_scores = [float(row[5]) for row in table_data]\n    print(f\"Average AUC: {sum(auc_scores)/len(auc_scores):.4f}\")\n    print(f\"Best AUC: {max(auc_scores):.4f} ({table_data[0][0]} model)\")\n\n    # Create and save visualization\n    fig = plot_comparison(results)\n    plt.savefig('performance_comparison.png')\n    plt.close()\n\n    print(\"\\nVisualization has been saved to 'performance_comparison.png'\")\n\nif __name__ == \"__main__\":\n    main()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T11:17:03.816616Z","iopub.execute_input":"2025-03-23T11:17:03.816951Z","iopub.status.idle":"2025-03-23T11:17:04.58475Z","shell.execute_reply.started":"2025-03-23T11:17:03.816919Z","shell.execute_reply":"2025-03-23T11:17:04.583985Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import optuna\nimport numpy as np\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import StandardScaler\nimport joblib\nimport json\nfrom train import preprocess_data\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\ndef optimize_random_forest(trial, X, y):\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n        'max_depth': trial.suggest_int('max_depth', 5, 30),\n        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n        'max_features': trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n        'class_weight': trial.suggest_categorical('class_weight', ['balanced', 'balanced_subsample', None]),\n        'random_state': 42\n    }\n    model = RandomForestClassifier(**params)\n    return cross_val_score(model, X, y, cv=5, scoring='accuracy').mean()\n\ndef optimize_svm(trial, X, y):\n    params = {\n        'C': trial.suggest_loguniform('C', 1e-3, 1e3),\n        'gamma': trial.suggest_loguniform('gamma', 1e-3, 1e3),\n        'kernel': trial.suggest_categorical('kernel', ['rbf', 'sigmoid', 'poly']),\n        'class_weight': trial.suggest_categorical('class_weight', ['balanced', None]),\n        'random_state': 42,\n        'probability': True\n    }\n    model = SVC(**params)\n    return cross_val_score(model, X, y, cv=5, scoring='accuracy').mean()\n\ndef optimize_xgboost(trial, X, y):\n    params = {\n        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.3),\n        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n        'max_depth': trial.suggest_int('max_depth', 3, 15),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 7),\n        'subsample': trial.suggest_uniform('subsample', 0.6, 1.0),\n        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.6, 1.0),\n        'scale_pos_weight': trial.suggest_loguniform('scale_pos_weight', 1, 10),\n        'random_state': 42\n    }\n    model = XGBClassifier(**params)\n    return cross_val_score(model, X, y, cv=5, scoring='accuracy').mean()\n\ndef optimize_gbm(trial, X, y):\n    params = {\n        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-3, 0.1),\n        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n        'max_depth': trial.suggest_int('max_depth', 3, 15),\n        'min_samples_split': trial.suggest_int('min_samples_split', 2, 20),\n        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 10),\n        'random_state': 42\n    }\n    model = GradientBoostingClassifier(**params)\n    return cross_val_score(model, X, y, cv=5, scoring='roc_auc').mean()\n\ndef optimize_mlp(trial, X, y):\n    layers = trial.suggest_int('n_layers', 1, 3)\n    params = {\n        'hidden_layer_sizes': tuple(\n            trial.suggest_int(f'n_units_l{i}', 32, 256) for i in range(layers)\n        ),\n        'learning_rate_init': trial.suggest_loguniform('learning_rate_init', 1e-4, 1e-1),\n        'max_iter': 1000,\n        'random_state': 42\n    }\n    model = MLPClassifier(**params)\n    return cross_val_score(model, X, y, cv=5, scoring='roc_auc').mean()\n\ndef optimize_knn(trial, X, y):\n    params = {\n        'n_neighbors': trial.suggest_int('n_neighbors', 1, 20),\n        'weights': trial.suggest_categorical('weights', ['uniform', 'distance']),\n        'p': trial.suggest_int('p', 1, 2)  # 1 for manhattan, 2 for euclidean\n    }\n    model = KNeighborsClassifier(**params)\n    return cross_val_score(model, X, y, cv=5, scoring='roc_auc').mean()\n\ndef main():\n    # Load and preprocess data\n    data = pd.read_csv(\"/kaggle/input/glaucoma/glaucoma_dataset.csv\", skipinitialspace=True)\n    processed_data = preprocess_data(data)\n    X = processed_data.values\n    y = (data['Diagnosis'] == 'Glaucoma').astype(int).values\n\n    # Scale the features\n    scaler = StandardScaler()\n    X_scaled = scaler.fit_transform(X)\n\n    # Dictionary of optimization functions\n    optimization_funcs = {\n        'Random_Forest': optimize_random_forest,\n        'SVM': optimize_svm,\n        'XGBoost': optimize_xgboost,\n        'GBM': optimize_gbm,\n        'Neural_Network': optimize_mlp,\n        'KNN': optimize_knn\n    }\n\n    # Store best parameters and scores\n    best_params = {}\n    best_scores = {}\n\n    # Run optimization for each model\n    n_trials = 100  # Increased from 50 to 100\n    for model_name, optimize_func in optimization_funcs.items():\n        print(f\"\\nOptimizing {model_name}...\")\n        study = optuna.create_study(direction='maximize')\n        study.optimize(lambda trial: optimize_func(trial, X_scaled, y), \n                      n_trials=n_trials)\n        \n        best_params[model_name] = study.best_params\n        best_scores[model_name] = study.best_value\n        \n        print(f\"Best {model_name} parameters:\", study.best_params)\n        print(f\"Best {model_name} score:\", study.best_value)\n\n    # Save results\n    results = {\n        'best_parameters': best_params,\n        'best_scores': best_scores\n    }\n    \n    with open('hyperparameter_optimization_results.json', 'w') as f:\n        json.dump(results, f, indent=4)\n\n    print(\"\\nOptimization results saved to hyperparameter_optimization_results.json\")\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T11:17:04.586186Z","iopub.execute_input":"2025-03-23T11:17:04.586404Z","iopub.status.idle":"2025-03-23T11:17:04.614191Z","shell.execute_reply.started":"2025-03-23T11:17:04.586386Z","shell.execute_reply":"2025-03-23T11:17:04.612538Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### logistic regression:\n\nfrom sklearn.linear_model import LogisticRegression\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline as ImbPipeline\nfrom sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\nimport joblib\nimport numpy as np\nimport json\n\ndef load_best_params():\n    try:\n        with open('fast_hyperopt_results.json', 'r') as f:\n            return json.load(f)['best_parameters'].get('Logistic_Regression', {})\n    except FileNotFoundError:\n        try:\n            with open('hyperparameter_optimization_results.json', 'r') as f:\n                return json.load(f)['best_parameters'].get('Logistic_Regression', {})\n        except FileNotFoundError:\n            return {}\n\ndef train(X_train, y_train):\n    # Load best parameters if available\n    best_params = load_best_params()\n    default_params = {'max_iter': 2000}\n    model_params = {**default_params, **best_params}\n    \n    # Create pipeline with SMOTE and model\n    pipeline = ImbPipeline([\n        ('sampling', SMOTE(random_state=42)),\n        ('model', LogisticRegression(**model_params))\n    ])\n    \n    # Train model\n    print(\"\\nTraining Logistic Regression...\")\n    pipeline.fit(X_train, y_train)\n    \n    # Save model\n    model_path = 'glaucoma_model_Logistic_Regression.joblib'\n    joblib.dump(pipeline, model_path)\n    print(f\"Model saved to {model_path}\")\n    \n    return pipeline\n\ndef evaluate(model, X, y):\n    # Get predictions\n    y_pred = model.predict(X)\n    \n    # Get probability predictions if available\n    if hasattr(model, 'predict_proba'):\n        y_prob = model.predict_proba(X)[:, 1]\n        # Convert probability predictions to binary if needed\n        if np.any((y_pred > 1) | (y_pred < 0)):\n            y_pred = (y_prob >= 0.5).astype(int)\n    else:\n        y_prob = y_pred\n        y_pred = (y_pred >= 0.5).astype(int)\n    \n    # Calculate metrics\n    metrics = {\n        'accuracy': accuracy_score(y, y_pred),\n        'precision': precision_score(y, y_pred),\n        'recall': recall_score(y, y_pred),\n        'f1': f1_score(y, y_pred),\n        'auc': roc_auc_score(y, y_prob)\n    }\n    \n    return metrics\n\ndef predict(model, X):\n    return model.predict(X)\n\ndef predict_proba(model, X):\n    if hasattr(model, 'predict_proba'):\n        return model.predict_proba(X)\n    return model.predict(X)\n\ndef load_model(filepath='glaucoma_model_Logistic_Regression.joblib'):\n    return joblib.load(filepath)\n\ndef main():\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import StandardScaler\n    import pandas as pd\n    \n    # Load and preprocess data (simplified example)\n    # In a real application, you would use the same preprocessing as in train.py\n    data = pd.read_csv(\"/kaggle/input/glaucoma/glaucoma_dataset.csv\")\n    X = data.drop('Diagnosis', axis=1).select_dtypes(include=['number']).fillna(0)\n    y = (data['Diagnosis'] == 'Glaucoma').astype(int)\n    \n    # Split data\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n    \n    # Scale data\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_val_scaled = scaler.transform(X_val)\n    \n    # Train model\n    model = train(X_train_scaled, y_train)\n    \n    # Evaluate model\n    metrics = evaluate(model, X_val_scaled, y_val)\n    print(\"\\nModel Performance:\")\n    for metric, value in metrics.items():\n        print(f\"{metric}: {value:.4f}\")\n    \nif __name__ == \"__main__\":\n    main()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T11:17:25.506434Z","iopub.execute_input":"2025-03-23T11:17:25.506753Z","iopub.status.idle":"2025-03-23T11:17:25.629359Z","shell.execute_reply.started":"2025-03-23T11:17:25.50673Z","shell.execute_reply":"2025-03-23T11:17:25.628523Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### decision tree:\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline as ImbPipeline\nfrom sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\nimport joblib\nimport numpy as np\nimport json\n\ndef load_best_params():\n    try:\n        with open('fast_hyperopt_results.json', 'r') as f:\n            return json.load(f)['best_parameters'].get('Decision_Tree', {})\n    except FileNotFoundError:\n        try:\n            with open('hyperparameter_optimization_results.json', 'r') as f:\n                return json.load(f)['best_parameters'].get('Decision_Tree', {})\n        except FileNotFoundError:\n            return {}\n\ndef train(X_train, y_train):\n    # Load best parameters if available\n    best_params = load_best_params()\n    default_params = {'random_state': 42}\n    model_params = {**default_params, **best_params}\n    \n    # Create pipeline with SMOTE and model\n    pipeline = ImbPipeline([\n        ('sampling', SMOTE(random_state=42)),\n        ('model', DecisionTreeClassifier(**model_params))\n    ])\n    \n    # Train model\n    print(\"\\nTraining Decision Tree...\")\n    pipeline.fit(X_train, y_train)\n    \n    # Save model\n    model_path = 'glaucoma_model_Decision_Tree.joblib'\n    joblib.dump(pipeline, model_path)\n    print(f\"Model saved to {model_path}\")\n    \n    return pipeline\n\ndef evaluate(model, X, y):\n    # Get predictions\n    y_pred = model.predict(X)\n    \n    # Get probability predictions if available\n    if hasattr(model, 'predict_proba'):\n        y_prob = model.predict_proba(X)[:, 1]\n        # Convert probability predictions to binary if needed\n        if np.any((y_pred > 1) | (y_pred < 0)):\n            y_pred = (y_prob >= 0.5).astype(int)\n    else:\n        y_prob = y_pred\n        y_pred = (y_pred >= 0.5).astype(int)\n    \n    # Calculate metrics\n    metrics = {\n        'accuracy': accuracy_score(y, y_pred),\n        'precision': precision_score(y, y_pred),\n        'recall': recall_score(y, y_pred),\n        'f1': f1_score(y, y_pred),\n        'auc': roc_auc_score(y, y_prob)\n    }\n    \n    return metrics\n\ndef predict(model, X):\n    return model.predict(X)\n\ndef predict_proba(model, X):\n    if hasattr(model, 'predict_proba'):\n        return model.predict_proba(X)\n    return model.predict(X)\n\ndef load_model(filepath='glaucoma_model_Decision_Tree.joblib'):\n    return joblib.load(filepath)\n\ndef main():\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import StandardScaler\n    import pandas as pd\n    \n    # Load and preprocess data (simplified example)\n    # In a real application, you would use the same preprocessing as in train.py\n    data = pd.read_csv(\"/kaggle/input/glaucoma/glaucoma_dataset.csv\")\n    X = data.drop('Diagnosis', axis=1).select_dtypes(include=['number']).fillna(0)\n    y = (data['Diagnosis'] == 'Glaucoma').astype(int)\n    \n    # Split data\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n    \n    # Scale data\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_val_scaled = scaler.transform(X_val)\n    \n    # Train model\n    model = train(X_train_scaled, y_train)\n    \n    # Evaluate model\n    metrics = evaluate(model, X_val_scaled, y_val)\n    print(\"\\nModel Performance:\")\n    for metric, value in metrics.items():\n        print(f\"{metric}: {value:.4f}\")\n    \nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T11:17:43.33562Z","iopub.execute_input":"2025-03-23T11:17:43.335948Z","iopub.status.idle":"2025-03-23T11:17:43.532095Z","shell.execute_reply.started":"2025-03-23T11:17:43.335921Z","shell.execute_reply":"2025-03-23T11:17:43.531428Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### random forest model:\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline as ImbPipeline\nfrom sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\nimport joblib\nimport numpy as np\nimport json\n\ndef load_best_params():\n    try:\n        with open('fast_hyperopt_results.json', 'r') as f:\n            return json.load(f)['best_parameters'].get('Random_Forest', {})\n    except FileNotFoundError:\n        try:\n            with open('hyperparameter_optimization_results.json', 'r') as f:\n                return json.load(f)['best_parameters'].get('Random_Forest', {})\n        except FileNotFoundError:\n            return {}\n\ndef train(X_train, y_train):\n    # Load best parameters if available\n    best_params = load_best_params()\n    default_params = {'random_state': 42}\n    model_params = {**default_params, **best_params}\n    \n    # Create pipeline with SMOTE and model\n    pipeline = ImbPipeline([\n        ('sampling', SMOTE(random_state=42)),\n        ('model', RandomForestClassifier(**model_params))\n    ])\n    \n    # Train model\n    print(\"\\nTraining Random Forest...\")\n    pipeline.fit(X_train, y_train)\n    \n    # Save model\n    model_path = 'glaucoma_model_Random_Forest.joblib'\n    joblib.dump(pipeline, model_path)\n    print(f\"Model saved to {model_path}\")\n    \n    return pipeline\n\ndef evaluate(model, X, y):\n    # Get predictions\n    y_pred = model.predict(X)\n    \n    # Get probability predictions if available\n    if hasattr(model, 'predict_proba'):\n        y_prob = model.predict_proba(X)[:, 1]\n        # Convert probability predictions to binary if needed\n        if np.any((y_pred > 1) | (y_pred < 0)):\n            y_pred = (y_prob >= 0.5).astype(int)\n    else:\n        y_prob = y_pred\n        y_pred = (y_pred >= 0.5).astype(int)\n    \n    # Calculate metrics\n    metrics = {\n        'accuracy': accuracy_score(y, y_pred),\n        'precision': precision_score(y, y_pred),\n        'recall': recall_score(y, y_pred),\n        'f1': f1_score(y, y_pred),\n        'auc': roc_auc_score(y, y_prob)\n    }\n    \n    return metrics\n\ndef predict(model, X):\n    return model.predict(X)\n\ndef predict_proba(model, X):\n    if hasattr(model, 'predict_proba'):\n        return model.predict_proba(X)\n    return model.predict(X)\n\ndef load_model(filepath='glaucoma_model_Random_Forest.joblib'):\n    return joblib.load(filepath)\n\ndef main():\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import StandardScaler\n    import pandas as pd\n    \n    # Load and preprocess data (simplified example)\n    # In a real application, you would use the same preprocessing as in train.py\n    data = pd.read_csv(\"/kaggle/input/glaucoma/glaucoma_dataset.csv\")\n    X = data.drop('Diagnosis', axis=1).select_dtypes(include=['number']).fillna(0)\n    y = (data['Diagnosis'] == 'Glaucoma').astype(int)\n    \n    # Split data\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n    \n    # Scale data\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_val_scaled = scaler.transform(X_val)\n    \n    # Train model\n    model = train(X_train_scaled, y_train)\n    \n    # Evaluate model\n    metrics = evaluate(model, X_val_scaled, y_val)\n    print(\"\\nModel Performance:\")\n    for metric, value in metrics.items():\n        print(f\"{metric}: {value:.4f}\")\n    \nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T11:17:52.606169Z","iopub.execute_input":"2025-03-23T11:17:52.6065Z","iopub.status.idle":"2025-03-23T11:17:54.794089Z","shell.execute_reply.started":"2025-03-23T11:17:52.606478Z","shell.execute_reply":"2025-03-23T11:17:54.79334Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"###XAI for Random Forest(seconf best)\nimport numpy as np\nimport pandas as pd\nimport joblib\nimport shap\nimport lime.lime_tabular\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.inspection import permutation_importance, PartialDependenceDisplay\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n# -------------------------------\n# 🔹 Load Data and Preprocess\n# -------------------------------\ndata = pd.read_csv(\"/kaggle/input/glaucoma/glaucoma_dataset.csv\")\nX = data.drop('Diagnosis', axis=1).select_dtypes(include=['number']).fillna(0)\ny = (data['Diagnosis'] == 'Glaucoma').astype(int)\n\n# Convert DataFrame to NumPy array for model compatibility\nX_numpy = X.values\n\n# Split Data\nX_train, X_test, y_train, y_test = train_test_split(X_numpy, y, test_size=0.2, random_state=42, stratify=y)\n\n# Scale Data\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Load Pretrained Random Forest Model\nrf_model = joblib.load(\"glaucoma_model_Random_Forest.joblib\")\n\n# Select a Sample for Explanation\nsample_idx = 10  # Pick a random test sample\nX_sample = X_test[sample_idx].reshape(1, -1)  # Convert single instance to NumPy array\n\n# -------------------------------\n# 🔹 1. Feature Importance (Random Forest)\n# -------------------------------\nfeature_importance = pd.Series(rf_model.named_steps['model'].feature_importances_, index=X.columns).sort_values(ascending=False)\nplt.figure(figsize=(10, 5))\nsns.barplot(x=feature_importance[:10], y=feature_importance.index[:10], palette=\"coolwarm\")\nplt.xlabel(\"Feature Importance Score\")\nplt.title(\"Top 10 Feature Importance (Random Forest)\")\nplt.show()\n\n# -------------------------------\n# 🔹 2. Partial Dependence Plot (PDP) - FIXED\n# -------------------------------\n# Use only features present in the dataset\nvalid_features = [f for f in feature_importance.index if f in X.columns][:2]  # Pick first 2 valid features\n\nif valid_features:\n    display = PartialDependenceDisplay.from_estimator(rf_model.named_steps['model'], X_train_scaled, features=[X.columns.get_loc(f) for f in valid_features], grid_resolution=50)\n    display.plot()\n    plt.suptitle(\"Partial Dependence Plots (Top 2 Features)\")\n    plt.show()\nelse:\n    print(\"No valid features found for Partial Dependence Plot.\")\n\n# -------------------------------\n# 🔹 3. Permutation Importance\n# -------------------------------\nperm_importance = permutation_importance(rf_model, X_test_scaled, y_test, scoring='accuracy', n_repeats=10, random_state=42)\nperm_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': perm_importance.importances_mean})\nperm_importance_df = perm_importance_df.sort_values(by='Importance', ascending=False)\n\nplt.figure(figsize=(10, 5))\nsns.barplot(x=perm_importance_df[\"Importance\"][:10], y=perm_importance_df[\"Feature\"][:10], palette=\"coolwarm\")\nplt.xlabel(\"Permutation Importance Score\")\nplt.title(\"Top 10 Permutation Importance (Random Forest)\")\nplt.show()\n\n# -------------------------------\n# 🔹 4. LIME Explanation (for one sample)\n# -------------------------------\nexplainer = lime.lime_tabular.LimeTabularExplainer(X_train_scaled, feature_names=X.columns, class_names=['No Glaucoma', 'Glaucoma'], discretize_continuous=True)\nexp = explainer.explain_instance(X_sample[0], rf_model.predict_proba, num_features=5)\nexp.show_in_notebook()\n\n# -------------------------------\n# 🔹 5. SHAP Explanation (Limited Visualizations)\n# -------------------------------\nexplainer = shap.Explainer(rf_model.predict_proba, X_train_scaled)\nshap_values = explainer(X_test_scaled)\n\n# Summary Plot (Top 10 features only)\nshap.summary_plot(shap_values[..., 1], X_test_scaled, feature_names=X.columns, max_display=10)  # Class 1 (Glaucoma)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T11:18:05.801307Z","iopub.execute_input":"2025-03-23T11:18:05.801634Z","iopub.status.idle":"2025-03-23T11:20:41.206139Z","shell.execute_reply.started":"2025-03-23T11:18:05.801608Z","shell.execute_reply":"2025-03-23T11:20:41.20529Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### svm model:\nfrom sklearn.svm import SVC\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline as ImbPipeline\nfrom sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\nimport joblib\nimport numpy as np\nimport json\n\ndef load_best_params():\n    try:\n        with open('fast_hyperopt_results.json', 'r') as f:\n            return json.load(f)['best_parameters'].get('SVM', {})\n    except FileNotFoundError:\n        try:\n            with open('hyperparameter_optimization_results.json', 'r') as f:\n                return json.load(f)['best_parameters'].get('SVM', {})\n        except FileNotFoundError:\n            return {}\n\ndef train(X_train, y_train):\n    # Load best parameters if available\n    best_params = load_best_params()\n    default_params = {'random_state': 42, 'probability': True}\n    model_params = {**default_params, **best_params}\n    \n    # Create pipeline with SMOTE and model\n    pipeline = ImbPipeline([\n        ('sampling', SMOTE(random_state=42)),\n        ('model', SVC(**model_params))\n    ])\n    \n    # Train model\n    print(\"\\nTraining SVM...\")\n    pipeline.fit(X_train, y_train)\n    \n    # Save model\n    model_path = 'glaucoma_model_SVM.joblib'\n    joblib.dump(pipeline, model_path)\n    print(f\"Model saved to {model_path}\")\n    \n    return pipeline\n\ndef evaluate(model, X, y):\n    # Get predictions\n    y_pred = model.predict(X)\n    \n    # Get probability predictions if available\n    if hasattr(model, 'predict_proba'):\n        y_prob = model.predict_proba(X)[:, 1]\n        # Convert probability predictions to binary if needed\n        if np.any((y_pred > 1) | (y_pred < 0)):\n            y_pred = (y_prob >= 0.5).astype(int)\n    else:\n        y_prob = y_pred\n        y_pred = (y_pred >= 0.5).astype(int)\n    \n    # Calculate metrics\n    metrics = {\n        'accuracy': accuracy_score(y, y_pred),\n        'precision': precision_score(y, y_pred),\n        'recall': recall_score(y, y_pred),\n        'f1': f1_score(y, y_pred),\n        'auc': roc_auc_score(y, y_prob)\n    }\n    \n    return metrics\n\ndef predict(model, X):\n    return model.predict(X)\n\ndef predict_proba(model, X):\n    if hasattr(model, 'predict_proba'):\n        return model.predict_proba(X)\n    return model.predict(X)\n\ndef load_model(filepath='glaucoma_model_SVM.joblib'):\n    return joblib.load(filepath)\n\ndef main():\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import StandardScaler\n    import pandas as pd\n    \n    # Load and preprocess data (simplified example)\n    # In a real application, you would use the same preprocessing as in train.py\n    data = pd.read_csv(\"/kaggle/input/glaucoma/glaucoma_dataset.csv\")\n    X = data.drop('Diagnosis', axis=1).select_dtypes(include=['number']).fillna(0)\n    y = (data['Diagnosis'] == 'Glaucoma').astype(int)\n    \n    # Split data\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n    \n    # Scale data\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_val_scaled = scaler.transform(X_val)\n    \n    # Train model\n    model = train(X_train_scaled, y_train)\n    \n    # Evaluate model\n    metrics = evaluate(model, X_val_scaled, y_val)\n    print(\"\\nModel Performance:\")\n    for metric, value in metrics.items():\n        print(f\"{metric}: {value:.4f}\")\n    \nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T11:20:51.664311Z","iopub.execute_input":"2025-03-23T11:20:51.664639Z","iopub.status.idle":"2025-03-23T11:21:11.182511Z","shell.execute_reply.started":"2025-03-23T11:20:51.664614Z","shell.execute_reply":"2025-03-23T11:21:11.181566Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"###XAI on SVM (second best)\nimport numpy as np\nimport pandas as pd\nimport joblib\nimport shap\nimport lime.lime_tabular\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.inspection import permutation_importance, PartialDependenceDisplay\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\n\n# -------------------------------\n# 🔹 Load Data and Preprocess\n# -------------------------------\ndata = pd.read_csv(\"/kaggle/input/glaucoma/glaucoma_dataset.csv\")\nX = data.drop('Diagnosis', axis=1).select_dtypes(include=['number']).fillna(0)\ny = (data['Diagnosis'] == 'Glaucoma').astype(int)\n\n# Convert DataFrame to NumPy array for model compatibility\nX_numpy = X.values\n\n# Split Data\nX_train, X_test, y_train, y_test = train_test_split(X_numpy, y, test_size=0.2, random_state=42, stratify=y)\n\n# Scale Data\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Load Pretrained SVM Model\nsvm_model = joblib.load(\"glaucoma_model_SVM.joblib\")\n\n# Select a Sample for Explanation\nsample_idx = 10  # Pick a random test sample\nX_sample = X_test[sample_idx].reshape(1, -1)  # Convert single instance to NumPy array\n\n# -------------------------------\n# 🔹 1. Feature Importance via Permutation Importance (SVM has no built-in feature importance)\n# -------------------------------\nperm_importance = permutation_importance(svm_model, X_test_scaled, y_test, scoring='accuracy', n_repeats=10, random_state=42)\nperm_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': perm_importance.importances_mean})\nperm_importance_df = perm_importance_df.sort_values(by='Importance', ascending=False)\n\nplt.figure(figsize=(10, 5))\nsns.barplot(x=perm_importance_df[\"Importance\"][:10], y=perm_importance_df[\"Feature\"][:10], palette=\"coolwarm\")\nplt.xlabel(\"Permutation Importance Score\")\nplt.title(\"Top 10 Feature Importance (SVM)\")\nplt.show()\n\n# -------------------------------\n# 🔹 2. Partial Dependence Plot (PDP) - FIXED\n# -------------------------------\n# Use only features present in the dataset\nvalid_features = perm_importance_df[\"Feature\"][:2].tolist()  # Pick first 2 valid features\n\nif valid_features:\n    display = PartialDependenceDisplay.from_estimator(svm_model, X_train_scaled, features=[X.columns.get_loc(f) for f in valid_features], grid_resolution=50)\n    display.plot()\n    plt.suptitle(\"Partial Dependence Plots (Top 2 Features)\")\n    plt.show()\nelse:\n    print(\"No valid features found for Partial Dependence Plot.\")\n\n# -------------------------------\n# 🔹 3. Permutation Importance (Direct Interpretation)\n# -------------------------------\nplt.figure(figsize=(10, 5))\nsns.barplot(x=perm_importance_df[\"Importance\"][:10], y=perm_importance_df[\"Feature\"][:10], palette=\"coolwarm\")\nplt.xlabel(\"Permutation Importance Score\")\nplt.title(\"Top 10 Permutation Importance (SVM Model)\")\nplt.show()\n\n# -------------------------------\n# 🔹 4. LIME Explanation (for one sample)\n# -------------------------------\nexplainer = lime.lime_tabular.LimeTabularExplainer(X_train_scaled, feature_names=X.columns, class_names=['No Glaucoma', 'Glaucoma'], discretize_continuous=True)\nexp = explainer.explain_instance(X_sample[0], svm_model.decision_function, num_features=5)\nexp.show_in_notebook()\n\n# -------------------------------\n# 🔹 5. SHAP Explanation (Limited Visualizations)\n# -------------------------------\nexplainer = shap.Explainer(svm_model.decision_function, X_train_scaled)\nshap_values = explainer(X_test_scaled)\n\n# Summary Plot (Top 10 features only)\nshap.summary_plot(shap_values, X_test_scaled, feature_names=X.columns, max_display=10)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T12:10:24.158554Z","iopub.execute_input":"2025-03-23T12:10:24.158893Z","iopub.status.idle":"2025-03-23T12:16:01.15296Z","shell.execute_reply.started":"2025-03-23T12:10:24.158871Z","shell.execute_reply":"2025-03-23T12:16:01.151743Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### knn model:\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline as ImbPipeline\nfrom sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\nimport joblib\nimport numpy as np\nimport json\n\ndef load_best_params():\n    try:\n        with open('fast_hyperopt_results.json', 'r') as f:\n            return json.load(f)['best_parameters'].get('KNN', {})\n    except FileNotFoundError:\n        try:\n            with open('hyperparameter_optimization_results.json', 'r') as f:\n                return json.load(f)['best_parameters'].get('KNN', {})\n        except FileNotFoundError:\n            return {}\n\ndef train(X_train, y_train):\n    # Load best parameters if available\n    best_params = load_best_params()\n    default_params = {'n_neighbors': 5}\n    model_params = {**default_params, **best_params}\n    \n    # Create pipeline with SMOTE and model\n    pipeline = ImbPipeline([\n        ('sampling', SMOTE(random_state=42)),\n        ('model', KNeighborsClassifier(**model_params))\n    ])\n    \n    # Train model\n    print(\"\\nTraining KNN...\")\n    pipeline.fit(X_train, y_train)\n    \n    # Save model\n    model_path = 'glaucoma_model_KNN.joblib'\n    joblib.dump(pipeline, model_path)\n    print(f\"Model saved to {model_path}\")\n    \n    return pipeline\n\ndef evaluate(model, X, y):\n    # Get predictions\n    y_pred = model.predict(X)\n    \n    # Get probability predictions if available\n    if hasattr(model, 'predict_proba'):\n        y_prob = model.predict_proba(X)[:, 1]\n        # Convert probability predictions to binary if needed\n        if np.any((y_pred > 1) | (y_pred < 0)):\n            y_pred = (y_prob >= 0.5).astype(int)\n    else:\n        y_prob = y_pred\n        y_pred = (y_pred >= 0.5).astype(int)\n    \n    # Calculate metrics\n    metrics = {\n        'accuracy': accuracy_score(y, y_pred),\n        'precision': precision_score(y, y_pred),\n        'recall': recall_score(y, y_pred),\n        'f1': f1_score(y, y_pred),\n        'auc': roc_auc_score(y, y_prob)\n    }\n    \n    return metrics\n\ndef predict(model, X):\n    return model.predict(X)\n\ndef predict_proba(model, X):\n    if hasattr(model, 'predict_proba'):\n        return model.predict_proba(X)\n    return model.predict(X)\n\ndef load_model(filepath='glaucoma_model_KNN.joblib'):\n    return joblib.load(filepath)\n\ndef main():\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import StandardScaler\n    import pandas as pd\n    \n    # Load and preprocess data (simplified example)\n    # In a real application, you would use the same preprocessing as in train.py\n    data = pd.read_csv(\"/kaggle/input/glaucoma/glaucoma_dataset.csv\")\n    X = data.drop('Diagnosis', axis=1).select_dtypes(include=['number']).fillna(0)\n    y = (data['Diagnosis'] == 'Glaucoma').astype(int)\n    \n    # Split data\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n    \n    # Scale data\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_val_scaled = scaler.transform(X_val)\n    \n    # Train model\n    model = train(X_train_scaled, y_train)\n    \n    # Evaluate model\n    metrics = evaluate(model, X_val_scaled, y_val)\n    print(\"\\nModel Performance:\")\n    for metric, value in metrics.items():\n        print(f\"{metric}: {value:.4f}\")\n    \nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T11:21:46.724892Z","iopub.execute_input":"2025-03-23T11:21:46.725225Z","iopub.status.idle":"2025-03-23T11:21:46.963934Z","shell.execute_reply.started":"2025-03-23T11:21:46.725196Z","shell.execute_reply":"2025-03-23T11:21:46.963088Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"knn_model = joblib.load(\"/content/glaucoma_model_KNN.joblib\")  # Update this path\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T13:20:33.995073Z","iopub.execute_input":"2025-03-23T13:20:33.995487Z","iopub.status.idle":"2025-03-23T13:20:34.013316Z","shell.execute_reply.started":"2025-03-23T13:20:33.995452Z","shell.execute_reply":"2025-03-23T13:20:34.012277Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### XAI on knn model(best performing model)\nimport numpy as np\nimport pandas as pd\nimport joblib\nimport shap\n\nimport lime.lime_tabular\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.inspection import permutation_importance, PartialDependenceDisplay\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n# -------------------------------\n# 🔹 Load Data and Preprocess\n# -------------------------------\ndata = pd.read_csv(\"/kaggle/input/glaucoma/glaucoma_dataset.csv\")\nX = data.drop('Diagnosis', axis=1).select_dtypes(include=['number']).fillna(0)\ny = (data['Diagnosis'] == 'Glaucoma').astype(int)\n\n# Convert DataFrame to NumPy array for KNN model\nX_numpy = X.values\n\n# Split Data\nX_train, X_test, y_train, y_test = train_test_split(X_numpy, y, test_size=0.2, random_state=42, stratify=y)\n\n# Scale Data\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Load Pretrained KNN Model\nknn_model = joblib.load(\"glaucoma_model_KNN.joblib\")\n\n# Train a Random Forest for Feature Importance (since KNN lacks built-in feature importance)\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\nrf_model.fit(X_train_scaled, y_train)\n\n# Select a Sample for Explanation\nsample_idx = 10  # Pick a random test sample\nX_sample = X_test[sample_idx].reshape(1, -1)  # Convert single instance to NumPy array\n\n# -------------------------------\n# 🔹 1. Feature Importance (Random Forest)\n# -------------------------------\nfeature_importance = pd.Series(rf_model.feature_importances_, index=X.columns).sort_values(ascending=False)\nplt.figure(figsize=(10, 5))\nsns.barplot(x=feature_importance[:10], y=feature_importance.index[:10], palette=\"coolwarm\")\nplt.xlabel(\"Feature Importance Score\")\nplt.title(\"Top 10 Feature Importance (Random Forest)\")\nplt.show()\n\n# -------------------------------\n# 🔹 2. Partial Dependence Plot (PDP) - FIXED\n# -------------------------------\n# Use only features present in the scaled dataset\nvalid_features = [f for f in feature_importance.index if f in X.columns][:2]  # Pick the first 2 valid features\n\nif valid_features:\n    display = PartialDependenceDisplay.from_estimator(rf_model, X_train_scaled, features=[X.columns.get_loc(f) for f in valid_features], grid_resolution=50)\n    display.plot()\n    plt.suptitle(\"Partial Dependence Plots (Top 2 Features)\")\n    plt.show()\nelse:\n    print(\"No valid features found for Partial Dependence Plot.\")\n\n# -------------------------------\n# 🔹 3. Permutation Importance\n# -------------------------------\nperm_importance = permutation_importance(knn_model, X_test_scaled, y_test, scoring='accuracy', n_repeats=10, random_state=42)\nperm_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': perm_importance.importances_mean})\nperm_importance_df = perm_importance_df.sort_values(by='Importance', ascending=False)\n\nplt.figure(figsize=(10, 5))\nsns.barplot(x=perm_importance_df[\"Importance\"][:10], y=perm_importance_df[\"Feature\"][:10], palette=\"coolwarm\")\nplt.xlabel(\"Permutation Importance Score\")\nplt.title(\"Top 10 Permutation Importance (KNN Model)\")\nplt.show()\n\n# -------------------------------\n# 🔹 4. LIME Explanation (for one sample)\n# -------------------------------\nexplainer = lime.lime_tabular.LimeTabularExplainer(X_train_scaled, feature_names=X.columns, class_names=['No Glaucoma', 'Glaucoma'], discretize_continuous=True)\nexp = explainer.explain_instance(X_sample[0], knn_model.predict_proba, num_features=5)\nexp.show_in_notebook()\n\n# -------------------------------\n# 🔹 5. SHAP Explanation (Limited Visualizations)\n# -------------------------------\nexplainer = shap.Explainer(knn_model.predict_proba, X_train_scaled)\nshap_values = explainer(X_test_scaled)\n\n# Summary Plot (Top 10 features only)\nshap.summary_plot(shap_values[..., 1], X_test_scaled, feature_names=X.columns, max_display=10)  # Class 1 (Glaucoma)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T13:17:18.630535Z","iopub.execute_input":"2025-03-23T13:17:18.630852Z","iopub.status.idle":"2025-03-23T13:17:18.728707Z","shell.execute_reply.started":"2025-03-23T13:17:18.630831Z","shell.execute_reply":"2025-03-23T13:17:18.727425Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### gbm model:\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline as ImbPipeline\nfrom sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\nimport joblib\nimport numpy as np\nimport json\n\ndef load_best_params():\n    try:\n        with open('fast_hyperopt_results.json', 'r') as f:\n            return json.load(f)['best_parameters'].get('GBM', {})\n    except FileNotFoundError:\n        try:\n            with open('hyperparameter_optimization_results.json', 'r') as f:\n                return json.load(f)['best_parameters'].get('GBM', {})\n        except FileNotFoundError:\n            return {}\n\ndef train(X_train, y_train):\n    # Load best parameters if available\n    best_params = load_best_params()\n    default_params = {'random_state': 42}\n    model_params = {**default_params, **best_params}\n    \n    # Create pipeline with SMOTE and model\n    pipeline = ImbPipeline([\n        ('sampling', SMOTE(random_state=42)),\n        ('model', GradientBoostingClassifier(**model_params))\n    ])\n    \n    # Train model\n    print(\"\\nTraining Gradient Boosting...\")\n    pipeline.fit(X_train, y_train)\n    \n    # Save model\n    model_path = 'glaucoma_model_GBM.joblib'\n    joblib.dump(pipeline, model_path)\n    print(f\"Model saved to {model_path}\")\n    \n    return pipeline\n\ndef evaluate(model, X, y):\n    # Get predictions\n    y_pred = model.predict(X)\n    \n    # Get probability predictions if available\n    if hasattr(model, 'predict_proba'):\n        y_prob = model.predict_proba(X)[:, 1]\n        # Convert probability predictions to binary if needed\n        if np.any((y_pred > 1) | (y_pred < 0)):\n            y_pred = (y_prob >= 0.5).astype(int)\n    else:\n        y_prob = y_pred\n        y_pred = (y_pred >= 0.5).astype(int)\n    \n    # Calculate metrics\n    metrics = {\n        'accuracy': accuracy_score(y, y_pred),\n        'precision': precision_score(y, y_pred),\n        'recall': recall_score(y, y_pred),\n        'f1': f1_score(y, y_pred),\n        'auc': roc_auc_score(y, y_prob)\n    }\n    \n    return metrics\n\ndef predict(model, X):\n    return model.predict(X)\n\ndef predict_proba(model, X):\n    if hasattr(model, 'predict_proba'):\n        return model.predict_proba(X)\n    return model.predict(X)\n\ndef load_model(filepath='glaucoma_model_GBM.joblib'):\n    return joblib.load(filepath)\n\ndef main():\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import StandardScaler\n    import pandas as pd\n    \n    # Load and preprocess data (simplified example)\n    # In a real application, you would use the same preprocessing as in train.py\n    data = pd.read_csv(\"/kaggle/input/glaucoma/glaucoma_dataset.csv\")\n    X = data.drop('Diagnosis', axis=1).select_dtypes(include=['number']).fillna(0)\n    y = (data['Diagnosis'] == 'Glaucoma').astype(int)\n    \n    # Split data\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n    \n    # Scale data\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_val_scaled = scaler.transform(X_val)\n    \n    # Train model\n    model = train(X_train_scaled, y_train)\n    \n    # Evaluate model\n    metrics = evaluate(model, X_val_scaled, y_val)\n    print(\"\\nModel Performance:\")\n    for metric, value in metrics.items():\n        print(f\"{metric}: {value:.4f}\")\n    \nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T11:24:20.489526Z","iopub.execute_input":"2025-03-23T11:24:20.489869Z","iopub.status.idle":"2025-03-23T11:24:21.640132Z","shell.execute_reply.started":"2025-03-23T11:24:20.489845Z","shell.execute_reply":"2025-03-23T11:24:21.639217Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"###XAI on GBM (worst performing model)\nimport numpy as np\nimport pandas as pd\nimport joblib\nimport shap\nimport lime.lime_tabular\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.inspection import permutation_importance, PartialDependenceDisplay\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n# -------------------------------\n# 🔹 Load Data and Preprocess\n# -------------------------------\ndata = pd.read_csv(\"/kaggle/input/glaucoma/glaucoma_dataset.csv\")\nX = data.drop('Diagnosis', axis=1).select_dtypes(include=['number']).fillna(0)\ny = (data['Diagnosis'] == 'Glaucoma').astype(int)\n\n# Convert DataFrame to NumPy array for model compatibility\nX_numpy = X.values\n\n# Split Data\nX_train, X_test, y_train, y_test = train_test_split(X_numpy, y, test_size=0.2, random_state=42, stratify=y)\n\n# Scale Data\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Load Pretrained GBM Model\ngbm_model = joblib.load(\"glaucoma_model_GBM.joblib\")\n\n# Select a Sample for Explanation\nsample_idx = 10  # Pick a random test sample\nX_sample = X_test[sample_idx].reshape(1, -1)  # Convert single instance to NumPy array\n\n# -------------------------------\n# 🔹 1. Feature Importance (GBM)\n# -------------------------------\nfeature_importance = pd.Series(gbm_model.named_steps['model'].feature_importances_, index=X.columns).sort_values(ascending=False)\nplt.figure(figsize=(10, 5))\nsns.barplot(x=feature_importance[:10], y=feature_importance.index[:10], palette=\"coolwarm\")\nplt.xlabel(\"Feature Importance Score\")\nplt.title(\"Top 10 Feature Importance (GBM)\")\nplt.show()\n\n# -------------------------------\n# 🔹 2. Partial Dependence Plot (PDP) - FIXED\n# -------------------------------\n# Use only features present in the dataset\nvalid_features = [f for f in feature_importance.index if f in X.columns][:2]  # Pick first 2 valid features\n\nif valid_features:\n    display = PartialDependenceDisplay.from_estimator(gbm_model.named_steps['model'], X_train_scaled, features=[X.columns.get_loc(f) for f in valid_features], grid_resolution=50)\n    display.plot()\n    plt.suptitle(\"Partial Dependence Plots (Top 2 Features)\")\n    plt.show()\nelse:\n    print(\"No valid features found for Partial Dependence Plot.\")\n\n# -------------------------------\n# 🔹 3. Permutation Importance\n# -------------------------------\nperm_importance = permutation_importance(gbm_model, X_test_scaled, y_test, scoring='accuracy', n_repeats=10, random_state=42)\nperm_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': perm_importance.importances_mean})\nperm_importance_df = perm_importance_df.sort_values(by='Importance', ascending=False)\n\nplt.figure(figsize=(10, 5))\nsns.barplot(x=perm_importance_df[\"Importance\"][:10], y=perm_importance_df[\"Feature\"][:10], palette=\"coolwarm\")\nplt.xlabel(\"Permutation Importance Score\")\nplt.title(\"Top 10 Permutation Importance (GBM)\")\nplt.show()\n\n# -------------------------------\n# 🔹 4. LIME Explanation (for one sample)\n# -------------------------------\nexplainer = lime.lime_tabular.LimeTabularExplainer(X_train_scaled, feature_names=X.columns, class_names=['No Glaucoma', 'Glaucoma'], discretize_continuous=True)\nexp = explainer.explain_instance(X_sample[0], gbm_model.predict_proba, num_features=5)\nexp.show_in_notebook()\n\n# -------------------------------\n# 🔹 5. SHAP Explanation (Limited Visualizations)\n# -------------------------------\nexplainer = shap.Explainer(gbm_model.predict_proba, X_train_scaled)\nshap_values = explainer(X_test_scaled)\n\n# Summary Plot (Top 10 features only)\nshap.summary_plot(shap_values[..., 1], X_test_scaled, feature_names=X.columns, max_display=10)  # Class 1 (Glaucoma)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T11:24:29.279699Z","iopub.execute_input":"2025-03-23T11:24:29.280002Z","iopub.status.idle":"2025-03-23T11:24:38.366363Z","shell.execute_reply.started":"2025-03-23T11:24:29.279979Z","shell.execute_reply":"2025-03-23T11:24:38.365551Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### xgboost model:\n\nfrom xgboost import XGBClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline as ImbPipeline\nfrom sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\nimport joblib\nimport numpy as np\nimport json\n\ndef load_best_params():\n    try:\n        with open('fast_hyperopt_results.json', 'r') as f:\n            return json.load(f)['best_parameters'].get('XGBoost', {})\n    except FileNotFoundError:\n        try:\n            with open('hyperparameter_optimization_results.json', 'r') as f:\n                return json.load(f)['best_parameters'].get('XGBoost', {})\n        except FileNotFoundError:\n            return {}\n\ndef train(X_train, y_train):\n    # Load best parameters if available\n    best_params = load_best_params()\n    default_params = {\n        'random_state': 42,\n        'objective': 'binary:logistic',\n        'use_label_encoder': False,\n        'eval_metric': 'logloss'\n    }\n    model_params = {**default_params, **best_params}\n    \n    # Create pipeline with SMOTE and model\n    pipeline = ImbPipeline([\n        ('sampling', SMOTE(random_state=42)),\n        ('model', XGBClassifier(**model_params))\n    ])\n    \n    # Train model\n    print(\"\\nTraining XGBoost...\")\n    pipeline.fit(X_train, y_train)\n    \n    # Save model\n    model_path = 'glaucoma_model_XGBoost.joblib'\n    joblib.dump(pipeline, model_path)\n    print(f\"Model saved to {model_path}\")\n    \n    return pipeline\n\ndef evaluate(model, X, y):\n    # Get predictions\n    y_pred = model.predict(X)\n    \n    # Get probability predictions if available\n    if hasattr(model, 'predict_proba'):\n        y_prob = model.predict_proba(X)[:, 1]\n        # Convert probability predictions to binary if needed\n        if np.any((y_pred > 1) | (y_pred < 0)):\n            y_pred = (y_prob >= 0.5).astype(int)\n    else:\n        y_prob = y_pred\n        y_pred = (y_pred >= 0.5).astype(int)\n    \n    # Calculate metrics\n    metrics = {\n        'accuracy': accuracy_score(y, y_pred),\n        'precision': precision_score(y, y_pred),\n        'recall': recall_score(y, y_pred),\n        'f1': f1_score(y, y_pred),\n        'auc': roc_auc_score(y, y_prob)\n    }\n    \n    return metrics\n\ndef predict(model, X):\n    return model.predict(X)\n\ndef predict_proba(model, X):\n    if hasattr(model, 'predict_proba'):\n        return model.predict_proba(X)\n    return model.predict(X)\n\ndef load_model(filepath='glaucoma_model_XGBoost.joblib'):\n    return joblib.load(filepath)\n\ndef main():\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import StandardScaler\n    import pandas as pd\n    \n    # Load and preprocess data (simplified example)\n    # In a real application, you would use the same preprocessing as in train.py\n    data = pd.read_csv(\"/kaggle/input/glaucoma/glaucoma_dataset.csv\")\n    X = data.drop('Diagnosis', axis=1).select_dtypes(include=['number']).fillna(0)\n    y = (data['Diagnosis'] == 'Glaucoma').astype(int)\n    \n    # Split data\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n    \n    # Scale data\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_val_scaled = scaler.transform(X_val)\n    \n    # Train model\n    model = train(X_train_scaled, y_train)\n    \n    # Evaluate model\n    metrics = evaluate(model, X_val_scaled, y_val)\n    print(\"\\nModel Performance:\")\n    for metric, value in metrics.items():\n        print(f\"{metric}: {value:.4f}\")\n    \nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T11:28:11.711851Z","iopub.execute_input":"2025-03-23T11:28:11.712186Z","iopub.status.idle":"2025-03-23T11:28:11.943851Z","shell.execute_reply.started":"2025-03-23T11:28:11.712162Z","shell.execute_reply":"2025-03-23T11:28:11.942933Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### XAI on XGBoost model (second worst performing medel)\nimport numpy as np\nimport pandas as pd\nimport joblib\nimport shap\nimport lime.lime_tabular\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.inspection import permutation_importance, PartialDependenceDisplay\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n# -------------------------------\n# 🔹 Load Data and Preprocess\n# -------------------------------\ndata = pd.read_csv(\"/kaggle/input/glaucoma/glaucoma_dataset.csv\")\nX = data.drop('Diagnosis', axis=1).select_dtypes(include=['number']).fillna(0)\ny = (data['Diagnosis'] == 'Glaucoma').astype(int)\n\n# Convert DataFrame to NumPy array for model compatibility\nX_numpy = X.values\n\n# Split Data\nX_train, X_test, y_train, y_test = train_test_split(X_numpy, y, test_size=0.2, random_state=42, stratify=y)\n\n# Scale Data\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Load Pretrained XGBoost Model\nxgb_model = joblib.load(\"glaucoma_model_XGBoost.joblib\")\n\n# Select a Sample for Explanation\nsample_idx = 10  # Pick a random test sample\nX_sample = X_test[sample_idx].reshape(1, -1)  # Convert single instance to NumPy array\n\n# -------------------------------\n# 🔹 1. Feature Importance (XGBoost)\n# -------------------------------\nfeature_importance = pd.Series(xgb_model.named_steps['model'].feature_importances_, index=X.columns).sort_values(ascending=False)\nplt.figure(figsize=(10, 5))\nsns.barplot(x=feature_importance[:10], y=feature_importance.index[:10], palette=\"coolwarm\")\nplt.xlabel(\"Feature Importance Score\")\nplt.title(\"Top 10 Feature Importance (XGBoost)\")\nplt.show()\n\n# -------------------------------\n# 🔹 2. Partial Dependence Plot (PDP) - FIXED\n# -------------------------------\n# Use only features present in the dataset\nvalid_features = [f for f in feature_importance.index if f in X.columns][:2]  # Pick first 2 valid features\n\nif valid_features:\n    display = PartialDependenceDisplay.from_estimator(xgb_model.named_steps['model'], X_train_scaled, features=[X.columns.get_loc(f) for f in valid_features], grid_resolution=50)\n    display.plot()\n    plt.suptitle(\"Partial Dependence Plots (Top 2 Features)\")\n    plt.show()\nelse:\n    print(\"No valid features found for Partial Dependence Plot.\")\n\n# -------------------------------\n# 🔹 3. Permutation Importance\n# -------------------------------\nperm_importance = permutation_importance(xgb_model, X_test_scaled, y_test, scoring='accuracy', n_repeats=10, random_state=42)\nperm_importance_df = pd.DataFrame({'Feature': X.columns, 'Importance': perm_importance.importances_mean})\nperm_importance_df = perm_importance_df.sort_values(by='Importance', ascending=False)\n\nplt.figure(figsize=(10, 5))\nsns.barplot(x=perm_importance_df[\"Importance\"][:10], y=perm_importance_df[\"Feature\"][:10], palette=\"coolwarm\")\nplt.xlabel(\"Permutation Importance Score\")\nplt.title(\"Top 10 Permutation Importance (XGBoost)\")\nplt.show()\n\n# -------------------------------\n# 🔹 4. LIME Explanation (for one sample)\n# -------------------------------\nexplainer = lime.lime_tabular.LimeTabularExplainer(X_train_scaled, feature_names=X.columns, class_names=['No Glaucoma', 'Glaucoma'], discretize_continuous=True)\nexp = explainer.explain_instance(X_sample[0], xgb_model.predict_proba, num_features=5)\nexp.show_in_notebook()\n\n# -------------------------------\n# 🔹 5. SHAP Explanation (Limited Visualizations)\n# -------------------------------\nexplainer = shap.Explainer(xgb_model.predict_proba, X_train_scaled)\nshap_values = explainer(X_test_scaled)\n\n# Summary Plot (Top 10 features only)\nshap.summary_plot(shap_values[..., 1], X_test_scaled, feature_names=X.columns, max_display=10)  # Class 1 (Glaucoma)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T11:35:35.427752Z","iopub.execute_input":"2025-03-23T11:35:35.428057Z","execution_failed":"2025-03-23T12:55:29.419Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### neural network model:\n\nfrom sklearn.neural_network import MLPClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline as ImbPipeline\nfrom sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\nimport joblib\nimport numpy as np\nimport json\n\ndef load_best_params():\n    try:\n        with open('fast_hyperopt_results.json', 'r') as f:\n            return json.load(f)['best_parameters'].get('Neural_Network', {})\n    except FileNotFoundError:\n        try:\n            with open('hyperparameter_optimization_results.json', 'r') as f:\n                return json.load(f)['best_parameters'].get('Neural_Network', {})\n        except FileNotFoundError:\n            return {}\n\ndef train(X_train, y_train):\n    # Load best parameters if available\n    best_params = load_best_params()\n    default_params = {\n        'hidden_layer_sizes': (64, 32),\n        'max_iter': 1000,\n        'random_state': 42\n    }\n    model_params = {**default_params, **best_params}\n    \n    # Create pipeline with SMOTE and model\n    pipeline = ImbPipeline([\n        ('sampling', SMOTE(random_state=42)),\n        ('model', MLPClassifier(**model_params))\n    ])\n    \n    # Train model\n    print(\"\\nTraining Neural Network...\")\n    pipeline.fit(X_train, y_train)\n    \n    # Save model\n    model_path = 'glaucoma_model_Neural_Network.joblib'\n    joblib.dump(pipeline, model_path)\n    print(f\"Model saved to {model_path}\")\n    \n    return pipeline\n\ndef evaluate(model, X, y):\n    # Get predictions\n    y_pred = model.predict(X)\n    \n    # Get probability predictions if available\n    if hasattr(model, 'predict_proba'):\n        y_prob = model.predict_proba(X)[:, 1]\n        # Convert probability predictions to binary if needed\n        if np.any((y_pred > 1) | (y_pred < 0)):\n            y_pred = (y_prob >= 0.5).astype(int)\n    else:\n        y_prob = y_pred\n        y_pred = (y_pred >= 0.5).astype(int)\n    \n    # Calculate metrics\n    metrics = {\n        'accuracy': accuracy_score(y, y_pred),\n        'precision': precision_score(y, y_pred),\n        'recall': recall_score(y, y_pred),\n        'f1': f1_score(y, y_pred),\n        'auc': roc_auc_score(y, y_prob)\n    }\n    \n    return metrics\n\ndef predict(model, X):\n    return model.predict(X)\n\ndef predict_proba(model, X):\n    if hasattr(model, 'predict_proba'):\n        return model.predict_proba(X)\n    return model.predict(X)\n\ndef load_model(filepath='glaucoma_model_Neural_Network.joblib'):\n    return joblib.load(filepath)\n\ndef main():\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import StandardScaler\n    import pandas as pd\n    \n    # Load and preprocess data (simplified example)\n    # In a real application, you would use the same preprocessing as in train.py\n    data = pd.read_csv(\"/kaggle/input/glaucoma/glaucoma_dataset.csv\")\n    X = data.drop('Diagnosis', axis=1).select_dtypes(include=['number']).fillna(0)\n    y = (data['Diagnosis'] == 'Glaucoma').astype(int)\n    \n    # Split data\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n    \n    # Scale data\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_val_scaled = scaler.transform(X_val)\n    \n    # Train model\n    model = train(X_train_scaled, y_train)\n    \n    # Evaluate model\n    metrics = evaluate(model, X_val_scaled, y_val)\n    print(\"\\nModel Performance:\")\n    for metric, value in metrics.items():\n        print(f\"{metric}: {value:.4f}\")\n    \nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T11:41:55.190362Z","iopub.execute_input":"2025-03-23T11:41:55.190751Z","iopub.status.idle":"2025-03-23T11:42:06.4047Z","shell.execute_reply.started":"2025-03-23T11:41:55.19072Z","shell.execute_reply":"2025-03-23T11:42:06.403753Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"### autoencoder nn model:\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn.neural_network import MLPClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline as ImbPipeline\nfrom sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score, f1_score\nimport joblib\nimport numpy as np\nimport json\n\nclass GlaucomaDataset(Dataset):\n    def __init__(self, X, y):\n        self.X = torch.FloatTensor(X)\n        self.y = torch.LongTensor(y)\n        \n    def __len__(self):\n        return len(self.X)\n    \n    def __getitem__(self, idx):\n        return self.X[idx], self.y[idx]\n\nclass Autoencoder(nn.Module):\n    def __init__(self, input_size, encoding_dim=64):\n        super(Autoencoder, self).__init__()\n        \n        # Encoder layers\n        self.encoder = nn.Sequential(\n            nn.Linear(input_size, 256),\n            nn.ReLU(),\n            nn.Linear(256, 128),\n            nn.ReLU(),\n            nn.Linear(128, encoding_dim),\n            nn.ReLU()\n        )\n        \n        # Decoder layers\n        self.decoder = nn.Sequential(\n            nn.Linear(encoding_dim, 128),\n            nn.ReLU(),\n            nn.Linear(128, 256),\n            nn.ReLU(),\n            nn.Linear(256, input_size),\n            nn.Sigmoid()  # Output should be in the range [0, 1]\n        )\n\n    def forward(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return decoded\n\ndef train_autoencoder(X_train, num_epochs=50):\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    # Create dataset and dataloader\n    train_dataset = GlaucomaDataset(X_train, np.zeros(len(X_train)))  # Dummy labels\n    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n    \n    # Initialize model, optimizer, and loss function\n    autoencoder = Autoencoder(input_size=X_train.shape[1]).to(device)\n    optimizer = optim.Adam(autoencoder.parameters(), lr=0.001)\n    criterion = nn.MSELoss()\n    \n    # Train autoencoder\n    autoencoder.train()\n    for epoch in range(num_epochs):\n        train_loss = 0\n        for batch_X, _ in train_loader:\n            batch_X = batch_X.to(device)\n            \n            # Forward pass\n            outputs = autoencoder(batch_X)\n            loss = criterion(outputs, batch_X)\n            \n            # Backward and optimize\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            \n            train_loss += loss.item()\n        \n        print(f'Epoch [{epoch+1}/{num_epochs}], Autoencoder Loss: {train_loss/len(train_loader):.4f}')\n    \n    # Save autoencoder model\n    torch.save(autoencoder.state_dict(), 'glaucoma_autoencoder.pth')\n    \n    # Extract features using encoder\n    autoencoder.eval()\n    with torch.no_grad():\n        X_train_encoded = autoencoder.encoder(torch.FloatTensor(X_train).to(device)).cpu().numpy()\n    \n    return autoencoder, X_train_encoded\n\ndef load_best_params():\n    try:\n        with open('fast_hyperopt_results.json', 'r') as f:\n            return json.load(f)['best_parameters'].get('Autoencoder_NN', {})\n    except FileNotFoundError:\n        try:\n            with open('hyperparameter_optimization_results.json', 'r') as f:\n                return json.load(f)['best_parameters'].get('Autoencoder_NN', {})\n        except FileNotFoundError:\n            return {}\n\ndef train(X_train, y_train):\n    # First train the autoencoder\n    autoencoder, X_train_encoded = train_autoencoder(X_train)\n    \n    # Then train classifier on encoded features\n    best_params = load_best_params()\n    default_params = {\n        'hidden_layer_sizes': (32, 16, 32),\n        'activation': 'relu',\n        'max_iter': 1000,\n        'random_state': 42\n    }\n    model_params = {**default_params, **best_params}\n    \n    # Create pipeline with SMOTE and model\n    pipeline = ImbPipeline([\n        ('sampling', SMOTE(random_state=42)),\n        ('model', MLPClassifier(**model_params))\n    ])\n    \n    # Train model\n    print(\"\\nTraining Neural Network on Autoencoded Features...\")\n    pipeline.fit(X_train_encoded, y_train)\n    \n    # Save model\n    model_path = 'glaucoma_model_Autoencoder_NN.joblib'\n    joblib.dump(pipeline, model_path)\n    print(f\"Model saved to {model_path}\")\n    \n    # Return both autoencoder and classifier\n    return {'autoencoder': autoencoder, 'classifier': pipeline}\n\ndef evaluate(model, X, y):\n    # Extract features using autoencoder\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    autoencoder = model['autoencoder']\n    classifier = model['classifier']\n    \n    with torch.no_grad():\n        X_encoded = autoencoder.encoder(torch.FloatTensor(X).to(device)).cpu().numpy()\n    \n    # Get predictions\n    y_pred = classifier.predict(X_encoded)\n    \n    # Get probability predictions if available\n    if hasattr(classifier, 'predict_proba'):\n        y_prob = classifier.predict_proba(X_encoded)[:, 1]\n        # Convert probability predictions to binary if needed\n        if np.any((y_pred > 1) | (y_pred < 0)):\n            y_pred = (y_prob >= 0.5).astype(int)\n    else:\n        y_prob = y_pred\n        y_pred = (y_pred >= 0.5).astype(int)\n    \n    # Calculate metrics\n    metrics = {\n        'accuracy': accuracy_score(y, y_pred),\n        'precision': precision_score(y, y_pred),\n        'recall': recall_score(y, y_pred),\n        'f1': f1_score(y, y_pred),\n        'auc': roc_auc_score(y, y_prob)\n    }\n    \n    return metrics\n\ndef predict(model, X):\n    # Extract features using autoencoder\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    autoencoder = model['autoencoder']\n    classifier = model['classifier']\n    \n    with torch.no_grad():\n        X_encoded = autoencoder.encoder(torch.FloatTensor(X).to(device)).cpu().numpy()\n    \n    return classifier.predict(X_encoded)\n\ndef predict_proba(model, X):\n    # Extract features using autoencoder\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    autoencoder = model['autoencoder']\n    classifier = model['classifier']\n    \n    with torch.no_grad():\n        X_encoded = autoencoder.encoder(torch.FloatTensor(X).to(device)).cpu().numpy()\n    \n    if hasattr(classifier, 'predict_proba'):\n        return classifier.predict_proba(X_encoded)\n    return classifier.predict(X_encoded)\n\ndef load_model():\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    # Load autoencoder\n    input_size = 13  # Default size, will be overridden when loaded\n    autoencoder = Autoencoder(input_size)\n    autoencoder.load_state_dict(torch.load('glaucoma_autoencoder.pth', map_location=device))\n    autoencoder.to(device)\n    autoencoder.eval()\n    \n    # Load classifier\n    classifier = joblib.load('glaucoma_model_Autoencoder_NN.joblib')\n    \n    return {'autoencoder': autoencoder, 'classifier': classifier}\n\ndef main():\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import StandardScaler\n    import pandas as pd\n    \n    # Load and preprocess data (simplified example)\n    # In a real application, you would use the same preprocessing as in train.py\n    data = pd.read_csv(\"/kaggle/input/glaucoma/glaucoma_dataset.csv\")\n    X = data.drop('Diagnosis', axis=1).select_dtypes(include=['number']).fillna(0)\n    y = (data['Diagnosis'] == 'Glaucoma').astype(int)\n    \n    # Split data\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n    \n    # Scale data\n    scaler = StandardScaler()\n    X_train_scaled = scaler.fit_transform(X_train)\n    X_val_scaled = scaler.transform(X_val)\n    \n    # Train model\n    model = train(X_train_scaled, y_train)\n    \n    # Evaluate model\n    metrics = evaluate(model, X_val_scaled, y_val)\n    print(\"\\nModel Performance:\")\n    for metric, value in metrics.items():\n        print(f\"{metric}: {value:.4f}\")\n    \nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-23T11:17:04.629427Z","iopub.status.idle":"2025-03-23T11:17:04.629807Z","shell.execute_reply":"2025-03-23T11:17:04.629642Z"}},"outputs":[],"execution_count":null}]}